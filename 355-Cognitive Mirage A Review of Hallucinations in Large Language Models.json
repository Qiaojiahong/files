{
    "pdf_info": [
        {
            "para_blocks": [
                {
                    "bbox": [
                        70,
                        75,
                        525,
                        94
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                70,
                                75,
                                525,
                                94
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        70,
                                        75,
                                        525,
                                        94
                                    ],
                                    "type": "text",
                                    "content": "Cognitive Mirage: A Review of Hallucinations in Large Language Models"
                                }
                            ]
                        }
                    ],
                    "index": 0
                },
                {
                    "bbox": [
                        145,
                        119,
                        452,
                        148
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                145,
                                119,
                                452,
                                148
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        145,
                                        119,
                                        452,
                                        148
                                    ],
                                    "type": "text",
                                    "content": "Hongbin Ye, Tong Liu, Aijia Zhang, Wei Hua, Weiqiang Jia Zhejiang Lab"
                                }
                            ]
                        }
                    ],
                    "index": 1
                },
                {
                    "bbox": [
                        158,
                        149,
                        440,
                        163
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                158,
                                149,
                                440,
                                163
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        158,
                                        149,
                                        440,
                                        163
                                    ],
                                    "type": "text",
                                    "content": "{yehongbin,jiaweiqiang}@zhejianglab.com"
                                }
                            ]
                        }
                    ],
                    "index": 2
                },
                {
                    "bbox": [
                        155,
                        212,
                        202,
                        225
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                155,
                                212,
                                202,
                                225
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        155,
                                        212,
                                        202,
                                        225
                                    ],
                                    "type": "text",
                                    "content": "Abstract"
                                }
                            ]
                        }
                    ],
                    "index": 3
                },
                {
                    "bbox": [
                        84,
                        234,
                        274,
                        485
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                84,
                                234,
                                274,
                                485
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        84,
                                        234,
                                        274,
                                        485
                                    ],
                                    "type": "text",
                                    "content": "As large language models continue to develop in the field of AI, text generation systems are susceptible to a worrisome phenomenon known as hallucination. In this study, we summarize recent compelling insights into hallucinations in LLMs. We present a novel taxonomy of hallucinations from various text generation tasks, thus provide theoretical insights, detection methods and improvement approaches. Based on this, future research directions are proposed. Our contribution are threefold: (1) We provide a detailed and complete taxonomy for hallucinations appearing in text generation tasks; (2) We provide theoretical analyses of hallucinations in LLMs and provide existing detection and improvement methods; (3) We propose several research directions that can be developed in the future. As hallucinations garner significant attention from the community, we will maintain updates on relevant research progress<sup>1</sup>."
                                }
                            ]
                        }
                    ],
                    "index": 4
                },
                {
                    "bbox": [
                        68,
                        493,
                        154,
                        507
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                68,
                                493,
                                154,
                                507
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        68,
                                        493,
                                        154,
                                        507
                                    ],
                                    "type": "text",
                                    "content": "1 Introduction"
                                }
                            ]
                        }
                    ],
                    "index": 5
                },
                {
                    "bbox": [
                        67,
                        515,
                        291,
                        745
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                67,
                                515,
                                291,
                                745
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        67,
                                        515,
                                        291,
                                        745
                                    ],
                                    "type": "text",
                                    "content": "In the ever-evolving realm of large language models (LLMs), a constellation of innovative creations has emerged, such as GPT-3 (Brown et al., 2020), InstructGPT (Ouyang et al., 2022), FLAN (Wei et al., 2022a), PaLM (Chowdhery et al., 2022), LLaMA (Touvron et al., 2023) and other notable contributors (Bai et al., 2022; Zhang et al., 2022; Zeng et al., 2023; Xu et al., 2023). These models implicitly encode global knowledge within their parameters during the pre-training phase (Han et al., 2021; Huang and Chang, 2023), offering valuable insights as knowledge repositories for downstream tasks (Pu and Demberg, 2023; Kojima et al., 2022; Wei et al., 2022b). Nevertheless, the generalization of knowledge can result in memory distortion, an inherent limitation that may give rise to potential inaccuracies (Yu et al., 2023b). Moreover, their"
                                }
                            ]
                        }
                    ],
                    "index": 6
                },
                {
                    "type": "image",
                    "bbox": [
                        309,
                        212,
                        527,
                        388
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                309,
                                212,
                                527,
                                388
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        309,
                                        212,
                                        527,
                                        388
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                309,
                                                212,
                                                527,
                                                388
                                            ],
                                            "type": "image",
                                            "image_path": "b8e40a7209e1c5ef4d591a25427c733a1bb35b4d7a8d133959c23d484be43346.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 7,
                            "angle": 0,
                            "type": "image_body"
                        },
                        {
                            "bbox": [
                                302,
                                396,
                                527,
                                444
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        302,
                                        396,
                                        527,
                                        444
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                302,
                                                396,
                                                527,
                                                444
                                            ],
                                            "type": "text",
                                            "content": "Figure 1: Illustration of Hallucination in LLMs. While the initial response appears fluent, it fails to align with the world knowledge retrieved from the external knowledge base."
                                        }
                                    ]
                                }
                            ],
                            "index": 8,
                            "angle": 0,
                            "type": "image_caption"
                        }
                    ],
                    "index": 7
                },
                {
                    "bbox": [
                        302,
                        476,
                        526,
                        773
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                302,
                                476,
                                526,
                                773
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        302,
                                        476,
                                        526,
                                        773
                                    ],
                                    "type": "text",
                                    "content": "ability to represent knowledge is constrained by model scale and faces challenges in addressing long-tailed knowledge problems (Kandpal et al., 2023; Mallen et al., 2023). While the privacy and timeliness of data in the real world (Lazaridou et al., 2022; Shi et al., 2023b) unfortunately exacerbate this problem, leaving models difficult to maintain a comprehensive and up-to-date understanding of the facts. These challenges present a serious obstacle to the reliability of LLMs, which we refer to as hallucination. (Yu et al., 2022). A prominent example of this drawback is that models typically generate statements that appear reasonable but are either cognitively irrelevant or factually incorrect. In light of this observation, hallucinations remain a critical challenge in medical (Dash et al., 2023; Umapathi et al., 2023), financial (Gill et al., 2023) and other knowledge-intensive fields due to the exacting accuracy requirements. Particularly, the applications for legal case drafting showcase plausible interpretation as an aggregation of diverse subjective perspectives (Curran et al., 2023)."
                                }
                            ]
                        }
                    ],
                    "index": 9
                }
            ],
            "discarded_blocks": [
                {
                    "bbox": [
                        13,
                        260,
                        36,
                        609
                    ],
                    "type": "aside_text",
                    "angle": 270,
                    "lines": [
                        {
                            "bbox": [
                                13,
                                260,
                                36,
                                609
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        13,
                                        260,
                                        36,
                                        609
                                    ],
                                    "type": "text",
                                    "content": "arXiv:2309.06794v1 [cs.CL] 13 Sep 2023"
                                }
                            ]
                        }
                    ],
                    "index": 10
                },
                {
                    "bbox": [
                        67,
                        750,
                        283,
                        773
                    ],
                    "type": "page_footnote",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                67,
                                750,
                                283,
                                773
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        67,
                                        750,
                                        283,
                                        773
                                    ],
                                    "type": "text",
                                    "content": "'https://github.com/hongbinye/Cognitive-Mirage-Hallucinations-in-LLMs"
                                }
                            ]
                        }
                    ],
                    "index": 11
                }
            ],
            "page_size": [
                595,
                841
            ],
            "page_idx": 0
        },
        {
            "para_blocks": [
                {
                    "type": "image",
                    "bbox": [
                        76,
                        71,
                        519,
                        259
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                76,
                                71,
                                519,
                                259
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        76,
                                        71,
                                        519,
                                        259
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                76,
                                                71,
                                                519,
                                                259
                                            ],
                                            "type": "image",
                                            "image_path": "ad802ceb06073426fa8bde34b3cde97bdf364c585c4aa0ccc7801794b49b899b.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 0,
                            "angle": 0,
                            "type": "image_body"
                        },
                        {
                            "bbox": [
                                67,
                                270,
                                526,
                                321
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        67,
                                        270,
                                        526,
                                        321
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                67,
                                                270,
                                                526,
                                                321
                                            ],
                                            "type": "text",
                                            "content": "Figure 2: The overview structure of this review. We firstly analyze three crucial factors that contribute to hallucinations and refine the categorization of hallucinations across text generation tasks. Subsequently, we dutifully report current methods for detecting and mitigating hallucinations. Finally, we propose several potential research directions to address evolving problems of hallucinations."
                                        }
                                    ]
                                }
                            ],
                            "index": 1,
                            "angle": 0,
                            "type": "image_caption"
                        }
                    ],
                    "index": 0
                },
                {
                    "bbox": [
                        67,
                        340,
                        291,
                        639
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                67,
                                340,
                                291,
                                639
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        67,
                                        340,
                                        291,
                                        639
                                    ],
                                    "type": "text",
                                    "content": "Definition of Hallucination. As depicted in Figure 1, hallucination refers to the generation of texts or responses that exhibit grammatical correctness, fluency, and authenticity, but deviate from the provided source inputs (faithfulness) or do not align with factual accuracy (factualness) (Ji et al., 2023). In traditional NLP tasks (Maynez et al., 2020), hallucinations are often synonymous with faithfulness: conflicting information leads to Intrinsic Hallucination, i.e., LMs conflict with the input information when generating a response; Conversely, generating ambiguous supplementary information may lead to Extrinsic Hallucination, i.e., LMs produce personal names, historical events, or technical documents that is challenging to verify. LLMs-oriented hallucinations instead prioritize factuality, focusing on whether the result can be evidenced or negated by reference to external facts in the real world. Uncritical trust in LLMs can give rise to a phenomenon Cognitive Mirage, contributing to misguided decision-making and a cascade of unintended consequences (Zhang et al., 2023a)."
                                }
                            ]
                        }
                    ],
                    "index": 2
                },
                {
                    "bbox": [
                        67,
                        645,
                        291,
                        713
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                67,
                                645,
                                291,
                                713
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        67,
                                        645,
                                        291,
                                        713
                                    ],
                                    "type": "text",
                                    "content": "Present work To effectively control the risk of hallucinations, we summarize recent progress in hallucination theories and solutions in this paper. We propose to organize relevant work by a comprehensive survey (Figure 2):"
                                }
                            ]
                        }
                    ],
                    "index": 3
                },
                {
                    "bbox": [
                        80,
                        719,
                        291,
                        774
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                80,
                                719,
                                291,
                                774
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        80,
                                        719,
                                        291,
                                        774
                                    ],
                                    "type": "text",
                                    "content": "- Theoretical insight and mechanism analysis. We provide in-depth theoretical and mechanism analysis from three typical perspectives: data collection, knowledge gap and optimiza"
                                }
                            ]
                        }
                    ],
                    "index": 4
                },
                {
                    "bbox": [
                        324,
                        340,
                        525,
                        367
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                324,
                                340,
                                525,
                                367
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        324,
                                        340,
                                        525,
                                        367
                                    ],
                                    "type": "text",
                                    "content": "tion process, reviewing the recent and relevant theories related to hallucinations (ยง2)."
                                }
                            ]
                        }
                    ],
                    "index": 5
                },
                {
                    "bbox": [
                        316,
                        375,
                        526,
                        547
                    ],
                    "type": "list",
                    "angle": 0,
                    "index": 8,
                    "blocks": [
                        {
                            "bbox": [
                                316,
                                375,
                                526,
                                444
                            ],
                            "type": "text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        316,
                                        375,
                                        526,
                                        444
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                316,
                                                375,
                                                526,
                                                444
                                            ],
                                            "type": "text",
                                            "content": "- Taxonomy of hallucination in LLMs. We conduct the comprehensive review of hallucination in LLMs together with task axis. We review the task-specific benchmarks with a comprehensive comparison and summary (ยง3)."
                                        }
                                    ]
                                }
                            ],
                            "index": 6
                        },
                        {
                            "bbox": [
                                316,
                                453,
                                525,
                                547
                            ],
                            "type": "text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        316,
                                        453,
                                        525,
                                        547
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                316,
                                                453,
                                                525,
                                                547
                                            ],
                                            "type": "text",
                                            "content": "- Wide coverage on emerging hallucination detection and correction methods. We propose a comprehensive investigation into the proactive detection ("
                                        },
                                        {
                                            "bbox": [
                                                316,
                                                453,
                                                525,
                                                547
                                            ],
                                            "type": "inline_equation",
                                            "content": "\\S 4"
                                        },
                                        {
                                            "bbox": [
                                                316,
                                                453,
                                                525,
                                                547
                                            ],
                                            "type": "text",
                                            "content": ") and mitigation of hallucinations ("
                                        },
                                        {
                                            "bbox": [
                                                316,
                                                453,
                                                525,
                                                547
                                            ],
                                            "type": "inline_equation",
                                            "content": "\\S 5"
                                        },
                                        {
                                            "bbox": [
                                                316,
                                                453,
                                                525,
                                                547
                                            ],
                                            "type": "text",
                                            "content": ") in the era of LLMs. This is critical to study the most popular techniques for inspiring future research directions ("
                                        },
                                        {
                                            "bbox": [
                                                316,
                                                453,
                                                525,
                                                547
                                            ],
                                            "type": "inline_equation",
                                            "content": "\\S 6"
                                        },
                                        {
                                            "bbox": [
                                                316,
                                                453,
                                                525,
                                                547
                                            ],
                                            "type": "text",
                                            "content": ")."
                                        }
                                    ]
                                }
                            ],
                            "index": 7
                        }
                    ],
                    "sub_type": "text"
                },
                {
                    "bbox": [
                        302,
                        556,
                        526,
                        773
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                302,
                                556,
                                526,
                                773
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        302,
                                        556,
                                        526,
                                        773
                                    ],
                                    "type": "text",
                                    "content": "Related work As this topic is relatively nascent, only a few surveys exist. Closest to our work, Ji et al. (2023) analyzes hallucinatory content in task-specific research progress, which focuses on early works in natural language generation field. Currently there are significant efforts to address hallucination in LLMs. Wang et al. (2023f) covers methods for effectively collecting high-quality instructions for LLM alignment, including the use of NLP benchmarks, human annotations, and leveraging strong LLMs. Pan et al. (2023) discusses self-correcting methods where LLM itself is prompted or guided to correct the hallucinations from its own outputs. Despite some benchmarks (Lin et al., 2022; Li et al., 2023b; Mihindukulasooriya et al., 2023) is constructed to evaluate whether LLMs"
                                }
                            ]
                        }
                    ],
                    "index": 9
                }
            ],
            "discarded_blocks": [],
            "page_size": [
                595,
                841
            ],
            "page_idx": 1
        },
        {
            "para_blocks": [
                {
                    "bbox": [
                        67,
                        71,
                        293,
                        167
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                67,
                                71,
                                293,
                                167
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        67,
                                        71,
                                        293,
                                        167
                                    ],
                                    "type": "text",
                                    "content": "are able to generate factual responses, these works scattered among various tasks have not been systematically reviewed and analyzed. Different from those surveys, in this paper, we conduct a literature review on hallucinations in LLMs, hoping to systematically understand the methodologies, compare different methods and inspire new ideas."
                                }
                            ]
                        }
                    ],
                    "index": 0
                },
                {
                    "bbox": [
                        67,
                        177,
                        196,
                        191
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                67,
                                177,
                                196,
                                191
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        67,
                                        177,
                                        196,
                                        191
                                    ],
                                    "type": "text",
                                    "content": "2 Mechanism Analysis"
                                }
                            ]
                        }
                    ],
                    "index": 1
                },
                {
                    "bbox": [
                        67,
                        199,
                        291,
                        293
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                67,
                                199,
                                291,
                                293
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        67,
                                        199,
                                        291,
                                        293
                                    ],
                                    "type": "text",
                                    "content": "For the sake of clean exposition, this section provides theoretical insight into mechanism analysis for hallucinations in LLMs. As a regular LLM, the generative objective is modeled by a parameterized probabilistic model "
                                },
                                {
                                    "bbox": [
                                        67,
                                        199,
                                        291,
                                        293
                                    ],
                                    "type": "inline_equation",
                                    "content": "p_{gen}"
                                },
                                {
                                    "bbox": [
                                        67,
                                        199,
                                        291,
                                        293
                                    ],
                                    "type": "text",
                                    "content": ", and sampled to predict the next token in the sentence, thus generating the entire sentence:"
                                }
                            ]
                        }
                    ],
                    "index": 2
                },
                {
                    "bbox": [
                        114,
                        306,
                        291,
                        321
                    ],
                    "type": "interline_equation",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                114,
                                306,
                                291,
                                321
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        114,
                                        306,
                                        291,
                                        321
                                    ],
                                    "type": "interline_equation",
                                    "content": "p _ {g e n} (y _ {i}) = \\mathcal {F} _ {\\boldsymbol {\\theta}} (\\mathcal {I}, \\mathcal {D}, x, y _ {i <  }) \\tag {1}",
                                    "image_path": "1c69d960dbbc072bee9c2a5ba8ce75ed35ffca9cf95c0554d531e267fdab8e32.jpg"
                                }
                            ]
                        }
                    ],
                    "index": 3
                },
                {
                    "bbox": [
                        67,
                        332,
                        291,
                        534
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                67,
                                332,
                                291,
                                534
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        67,
                                        332,
                                        291,
                                        534
                                    ],
                                    "type": "text",
                                    "content": "where "
                                },
                                {
                                    "bbox": [
                                        67,
                                        332,
                                        291,
                                        534
                                    ],
                                    "type": "inline_equation",
                                    "content": "y_{i}"
                                },
                                {
                                    "bbox": [
                                        67,
                                        332,
                                        291,
                                        534
                                    ],
                                    "type": "text",
                                    "content": " represents probable tokens at each step that can be selected by beam search from vocabulary "
                                },
                                {
                                    "bbox": [
                                        67,
                                        332,
                                        291,
                                        534
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathcal{V}"
                                },
                                {
                                    "bbox": [
                                        67,
                                        332,
                                        291,
                                        534
                                    ],
                                    "type": "text",
                                    "content": ". Note that the instructions "
                                },
                                {
                                    "bbox": [
                                        67,
                                        332,
                                        291,
                                        534
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathcal{I}"
                                },
                                {
                                    "bbox": [
                                        67,
                                        332,
                                        291,
                                        534
                                    ],
                                    "type": "text",
                                    "content": " utilize a variety of predefined templates according to different tasks (Yin et al., 2023a). Multifarious and high-quality in-context demonstrations "
                                },
                                {
                                    "bbox": [
                                        67,
                                        332,
                                        291,
                                        534
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathcal{D}"
                                },
                                {
                                    "bbox": [
                                        67,
                                        332,
                                        291,
                                        534
                                    ],
                                    "type": "text",
                                    "content": " are aimed at providing analogy samples to reduce the cost of adapting models to new tasks (Chen et al., 2022a). Parameters "
                                },
                                {
                                    "bbox": [
                                        67,
                                        332,
                                        291,
                                        534
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\theta"
                                },
                                {
                                    "bbox": [
                                        67,
                                        332,
                                        291,
                                        534
                                    ],
                                    "type": "text",
                                    "content": " implicitly memorize corpus knowledge through diverse architectural "
                                },
                                {
                                    "bbox": [
                                        67,
                                        332,
                                        291,
                                        534
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathcal{F}"
                                },
                                {
                                    "bbox": [
                                        67,
                                        332,
                                        291,
                                        534
                                    ],
                                    "type": "text",
                                    "content": " such as decoder-only, encoder-only, or encoder-decoder LLMs. As LLM-based systems can exhibit a variety of hallucinations, we summarise three primary mechanism types for theoretical analysis, and each mechanism is correlated with a distinct training factor."
                                }
                            ]
                        }
                    ],
                    "index": 4
                },
                {
                    "bbox": [
                        67,
                        543,
                        292,
                        774
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                67,
                                543,
                                292,
                                774
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        67,
                                        543,
                                        292,
                                        774
                                    ],
                                    "type": "text",
                                    "content": "Data Collection The parameters are implicitly stored within model as a priori knowledge acquired during pre-training process. Given the varying quality and range of knowledge within pre-trained corpus, the information incorporated into the LLMs may be incomplete or outdated. In cases where pertinent memories are unavailable, the LLM's performance may deteriorates, resorting to rudimentary corpus-based heuristics that rely on term frequencies to render judgements (McKenna et al., 2023). Another bias stems from the capacity for contextual learning (Chan et al., 2022) when a few demonstrations are introduced as input to the prefix context. Previous research (Wang et al., 2023d; Lu et al., 2022) has demonstrated that the acquisition of knowledge through model learning demonstrations depends on disparities in label categories and"
                                }
                            ]
                        }
                    ],
                    "index": 5
                },
                {
                    "bbox": [
                        302,
                        71,
                        527,
                        208
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                302,
                                71,
                                527,
                                208
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        302,
                                        71,
                                        527,
                                        208
                                    ],
                                    "type": "text",
                                    "content": "the order of demonstration samples. Likewise, multilingual LLMs encounter challenges related to hallucinations, particularly in handling language pairs with limited resources or non-English translations (Guerreiro et al., 2023a). Furthermore, cutting-edge Large Vision-Language Models (LVLMs) exhibit instances of hallucinating common objects within visual instructional datasets and prone to objects that frequently co-occur in the same image (Liu et al., 2023b; Li et al., 2023i)."
                                }
                            ]
                        }
                    ],
                    "index": 6
                },
                {
                    "bbox": [
                        302,
                        219,
                        527,
                        530
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                302,
                                219,
                                527,
                                530
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        302,
                                        219,
                                        527,
                                        530
                                    ],
                                    "type": "text",
                                    "content": "Knowledge Gap Knowledge gaps are typically attributed to differences in input format between the pre-training and fine-tuning stages (Zheng et al., 2023b). Even when considering the automatic updating of textual knowledge bases, the output can deviate from the expected corrections (Huang et al., 2023). For example, questions often do not align effectively with stored knowledge, and the available information remains unknown until the questions are presented. This knowledge gap poses thorny challenges in balancing memory with retrieved evidence, which is construed as a passive defense mechanism against the misuse of retrieval (Gao et al., 2023a). To in-depth analyses this issue, Zhang et al. (2023b) and Halawi et al. (2023) propose that disregarding retrieved evidence introduces biased model knowledge, while mis-covering and over-thinking disrupt model behavior. Furthermore, in scenarios where a cache component is utilized to offer historical memory during training (Wan et al., 2023), the model also experiences inconsistency between the present hidden state and the hidden state stored in the cache."
                                }
                            ]
                        }
                    ],
                    "index": 7
                },
                {
                    "bbox": [
                        302,
                        543,
                        527,
                        774
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                302,
                                543,
                                527,
                                774
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        302,
                                        543,
                                        527,
                                        774
                                    ],
                                    "type": "text",
                                    "content": "Optimization Process The maximum likelihood estimation and teacher-forcing training have the potential to result in a phenomenon known as stochastic parroting (Chiesurin et al., 2023), wherein the model is prompted to imitate the training data without comprehension (Kang and Hashimoto, 2020). Specifically, exposure bias between the training and testing stages have been demonstrated to lead to hallucinations within LLMs, particularly when generating lengthy responses (Wang and Sennrich, 2020). Besides, sampling techniques characterized by high uncertainty (Lee et al., 2022), such as top- and top-k, exacerbate the issue of hallucination. Furthermore, Zhang et al. (2023a) observes that LLMs tend to produce snowballing hallucinations to maintain coherence with earlier hallucinations, and even when directed with prompts as \"Let's"
                                }
                            ]
                        }
                    ],
                    "index": 8
                }
            ],
            "discarded_blocks": [],
            "page_size": [
                595,
                841
            ],
            "page_idx": 2
        },
        {
            "para_blocks": [
                {
                    "bbox": [
                        67,
                        71,
                        290,
                        98
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                67,
                                71,
                                290,
                                98
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        67,
                                        71,
                                        290,
                                        98
                                    ],
                                    "type": "text",
                                    "content": "think step by step\", they still generate ineffectual chains of reasoning (Kojima et al., 2022)."
                                }
                            ]
                        }
                    ],
                    "index": 0
                },
                {
                    "bbox": [
                        67,
                        111,
                        227,
                        125
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                67,
                                111,
                                227,
                                125
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        67,
                                        111,
                                        227,
                                        125
                                    ],
                                    "type": "text",
                                    "content": "3 Taxonomy of Hallucination"
                                }
                            ]
                        }
                    ],
                    "index": 1
                },
                {
                    "bbox": [
                        67,
                        134,
                        291,
                        269
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                67,
                                134,
                                291,
                                269
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        67,
                                        134,
                                        291,
                                        269
                                    ],
                                    "type": "text",
                                    "content": "In this paper, we mainly consider representative hallucinations, which are widely observed in various downstream tasks, i.e. Machine Translation, Question and Answer, Dialog System, Summarization System, Knowledge graph with LLMs, and Visual Question Answer. As shown in Table 1, these hallucinations are identified complex taxonomy in numerous mainstream tasks associated with LLMs. In the following sections, we will introduce representative types of hallucinations to be resolved."
                                }
                            ]
                        }
                    ],
                    "index": 2
                },
                {
                    "bbox": [
                        69,
                        270,
                        291,
                        514
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                69,
                                270,
                                291,
                                514
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        69,
                                        270,
                                        291,
                                        514
                                    ],
                                    "type": "text",
                                    "content": "- Machine Translation. Since perturbations (e.g., spellings or capital errors) can induce hallucinations reliably, traditional machine translation models tend to validate instances memorised by the model when subjected to perturbations (Bawden and Yvon, 2023; Hendy et al., 2023). It is worth noting that hallucinations generated by LLMs are mainly translation off-target, over-generation, or failed translation attempts (Guerreiro et al., 2023a). While in low-resource language setting, most models exhibit subpar performance due to the lack of annotated data (Dale et al., 2023). In contrast, they are vulnerable to increased amount of pre-trained languages in multilingual setting (Conneau et al., 2020). Subsequently, familial LLMs trained on different scales of monolingual data are proved to be viscous (Guerreiro et al., 2023a), as the source of oscillatory hallucination pathology."
                                }
                            ]
                        }
                    ],
                    "index": 3
                },
                {
                    "bbox": [
                        67,
                        515,
                        291,
                        717
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                67,
                                515,
                                291,
                                717
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        67,
                                        515,
                                        291,
                                        717
                                    ],
                                    "type": "text",
                                    "content": "- Question and Answer. Imperfect responses suffer from flawed external knowledge, knowledge recall cues and reasoning instruction (Zheng et al., 2023b). For example, LLMs are mostly unable to avoid answering when provided with no relevant information, instead provide incomplete and plausible answers (Adlakha et al., 2023). In addition to external knowledge, memorized information without accurate, reliable and accessible source also contributes to different types of hallucinations (Umapathi et al., 2023). Though scaling laws suggest that perplexity on the training distribution is positively correlated with parameter size, (Lin et al., 2022) further discovers that scaling up models should increase the rate of imitative falsehoods."
                                }
                            ]
                        }
                    ],
                    "index": 4
                },
                {
                    "bbox": [
                        67,
                        719,
                        291,
                        774
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                67,
                                719,
                                291,
                                774
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        67,
                                        719,
                                        291,
                                        774
                                    ],
                                    "type": "text",
                                    "content": "- Dialog System. Some studies view dialogue models as unobtrusive imitators, which simulates the distributional properties of data instead of generating faithful output. For example, Uncooper"
                                }
                            ]
                        }
                    ],
                    "index": 5
                },
                {
                    "bbox": [
                        302,
                        71,
                        526,
                        206
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                302,
                                71,
                                526,
                                206
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        302,
                                        71,
                                        526,
                                        206
                                    ],
                                    "type": "text",
                                    "content": "ativeness responses (Dziri et al., 2022b) originating from discourse phenomena inclines to output an exact copy of the entire evidence. Das et al. (2022) reports more nuanced hallucinations in KG-grounded dialogue systems as analyzed through human feedback. Similarly, FaithDial (Dziri et al., 2022a), BEGIN (Dziri et al., 2022c), MixCL (Sun et al., 2023) all implement experiments on the "
                                },
                                {
                                    "bbox": [
                                        302,
                                        71,
                                        526,
                                        206
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathsf{WOW}"
                                },
                                {
                                    "bbox": [
                                        302,
                                        71,
                                        526,
                                        206
                                    ],
                                    "type": "text",
                                    "content": " dataset to conduct a meta-evaluation of the hallucination in knowledge grounded dialogue."
                                }
                            ]
                        }
                    ],
                    "index": 6
                },
                {
                    "bbox": [
                        302,
                        211,
                        526,
                        468
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                302,
                                211,
                                526,
                                468
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        302,
                                        211,
                                        526,
                                        468
                                    ],
                                    "type": "text",
                                    "content": "- Summarization System. Automatically generated abstracts based on LLMs may be fluent, but they still typically lack faithfulness to the source document. Compared to the human evaluation of traditional summarization models (Maynez et al., 2020), the summarizations generated by LLMs can be categorized into two major types: intrinsic hallucinations that distort the information present in the document; extrinsic hallucinations that provide additional information that cannot be directly attributed to the document (Qiu et al., 2023). Note that extrinsic hallucination as a metrics of factually consistent continuation of inputs in LLMs is given more attention in summarisation systems (Tam et al., 2023; Shen et al., 2023). Furthermore, Cao et al. (2022) subdivides extrinsic hallucinations into factual and non-factual hallucinations. The former provides additional world knowledge, which may benefit comprehensive understanding."
                                }
                            ]
                        }
                    ],
                    "index": 7
                },
                {
                    "bbox": [
                        302,
                        471,
                        526,
                        741
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                302,
                                471,
                                526,
                                741
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        302,
                                        471,
                                        526,
                                        741
                                    ],
                                    "type": "text",
                                    "content": "- Knowledge Graph with LLMs. Despite the promising progress in knowledge-based text generation, it encounters intrinsic hallucinations inherent to the process where the generated text not only covers the input information but also incorporates redundant details derived from its internal memorized knowledge (Yuan and Fรคrber, 2023). To address this, Yu et al. (2023a) establish a distinction between correctly generated knowledge and knowledge hallucinations in terms of knowledge creation. Notably, the Virtual Knowledge Extraction (Zhu et al., 2023d) underscores the potential generalization capabilities of LLMs in the realms of constructing and inferring from knowledge graphs. Mihindukulasooriya et al. (2023) further empower LLMs to produce interpretable fact-checks through a neural symbolic approach. Based on their fidelity to the source, hallucinations are defined as subject hallucination, relation hallucination, and object hallucination."
                                }
                            ]
                        }
                    ],
                    "index": 8
                },
                {
                    "bbox": [
                        302,
                        746,
                        525,
                        772
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                302,
                                746,
                                525,
                                772
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        302,
                                        746,
                                        525,
                                        772
                                    ],
                                    "type": "text",
                                    "content": "- Cross-modal System. Augmented by the superior language capabilities of LLMs, performance"
                                }
                            ]
                        }
                    ],
                    "index": 9
                }
            ],
            "discarded_blocks": [],
            "page_size": [
                595,
                841
            ],
            "page_idx": 3
        },
        {
            "para_blocks": [
                {
                    "type": "table",
                    "bbox": [
                        77,
                        69,
                        515,
                        741
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                77,
                                69,
                                515,
                                741
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        77,
                                        69,
                                        515,
                                        741
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                77,
                                                69,
                                                515,
                                                741
                                            ],
                                            "type": "table",
                                            "html": "<table><tr><td>Paper</td><td>Task</td><td>Architecture</td><td>Resources</td><td>Hallucination Types</td><td>Research Method</td></tr><tr><td>Raunak et al. (2021)</td><td>Machine Translation</td><td>Enc-Dec</td><td>IWSLT-2014</td><td>Under perturbation, Natu-ral hallucination</td><td>Source perturbation</td></tr><tr><td>Guerreiro et al. (2023b)</td><td>Machine Translation</td><td>Enc-Dec</td><td>WMT2018</td><td>Oscillatory hallucination, Largely fluent hallucina-tion</td><td>Consider a natural scenario</td></tr><tr><td>Dale et al. (2023)</td><td>Machine Translation</td><td>Enc-Dec</td><td>FLORES-200, Jig-saw, Wikipedia</td><td>Full hallucination, Partial hallucination, Word-level hallucination</td><td>Introduce pathology detection</td></tr><tr><td>Pfeiffer et al. (2023)</td><td>Multilingual Seq2seq</td><td>Enc-Dec</td><td>XQuAD, TyDi, XNLI, XL-Sum, MASSIVE</td><td>Source language halluci-nation</td><td>Evaluate source lan-guage hallucination</td></tr><tr><td>Lin et al. (2022)</td><td>Question and Answer</td><td>Enc-Dec, Only-Dec</td><td>TruthfulQA</td><td>Imitative falsehoods</td><td>Cause imitative false-hoods</td></tr><tr><td>Zheng et al. (2023b)</td><td>Question and Answer</td><td>Only-Dec</td><td>HotpotQA, BoolQ</td><td>Comprehension, Factual-ness, Specificity, Infer-ence Hallucination</td><td>Manual analysis of re-sponses</td></tr><tr><td>Adlakha et al. (2023)</td><td>Question and Answer</td><td>Enc-Dec, Only-Dec</td><td>NQ, HotpotQA, TopiOCQA</td><td>Semantic equivalence, Symbolic equivalence, Intrinsic ambiguity, Gran-ularity discrepancies, Incomplete, Enumeration, Satisfactory Subset</td><td>Evaluate retrieval aug-mented QA</td></tr><tr><td>Umapathi et al. (2023)</td><td>Question and Answer</td><td>Only-Dec</td><td>MEDMCQA, Headqa, USMILE, Medqa, Pubmed</td><td>Reasoning hallucination, Memory-based hallucina-tion</td><td>Medical benchmark Med-HALT</td></tr><tr><td>Dziri et al. (2022b)</td><td>Dialog System</td><td>Enc-Dec, Only-Dec</td><td>WoW, CMU-DOG, TopicalChat</td><td>Hallucination, Partial hal-lucination, Generic, Un-cooperative</td><td>Infer exclusively from the knowledge-snippet</td></tr><tr><td>Das et al. (2022)</td><td>Dialog System</td><td>Only-Dec</td><td>OpenDialKG</td><td>Extrinsic-Soft/Hard/Grouped, Intrinsic-Soft/ Hard/Repetitive, History Corrupted</td><td>Analyze entity-level fact hallucination</td></tr><tr><td>Dziri et al. (2022a)</td><td>Dialog System</td><td>Enc-Dec, Only-Dec</td><td>WoW</td><td>Hallucination, Generic, Uncooperativeness</td><td>Hallucination-free benchmark FaithDial</td></tr><tr><td>Dziri et al. (2022c)</td><td>Dialog System</td><td>Enc-Dec, Only-Enc, Only-Dec</td><td>WoW, CMU-DOG, TopicalChat</td><td>Fully attributable, Not attributable, Generic</td><td>Knowledge-grounded interaction benchmark Begin</td></tr><tr><td>Sun et al. (2023)</td><td>Dialog System</td><td>Enc-Dec, Only-Dec</td><td>WoW</td><td>Intrinsic hallucination, Extrinsic hallucination</td><td>Sample responses for conversation</td></tr><tr><td>Tam et al. (2023)</td><td>Summarization System</td><td>Enc-Dec, Only-Dec</td><td>CNN/DM, XSum</td><td>Factually inconsistent summaries</td><td>Generate summaries from given models</td></tr><tr><td>Cao et al. (2022)</td><td>Summarization System</td><td>Enc-Dec, Only-Dec</td><td>MENT</td><td>Non-hallucinated, Factual hallucination, Non-factual hallucination, Intrinsic hallucination</td><td>Label factual entities from summarizations</td></tr><tr><td>Shen et al. (2023)</td><td>Summarization System</td><td>Enc-Dec, Only-Enc</td><td>NHNet</td><td>News headline hallucina-tion</td><td>Majority vote of jour-nalism degree holders</td></tr><tr><td>Qiu et al. (2023)</td><td>Summarization System</td><td>Multiple ADapters</td><td>XL-Sum</td><td>Intrinsic hallucination, Extrinsic hallucination</td><td>In a cross-lingual transfer setting</td></tr><tr><td>Yu et al. (2023a)</td><td>Knowledge-based text generation</td><td>Enc-Dec, Only-Dec</td><td>Encyclopedic, ETC</td><td>Knowledge hallucination</td><td>Evaluate knowledge creating ability given known facts</td></tr><tr><td>Mihindukulasooriy et al. (2023)</td><td>Knowledge graph generation</td><td>Only-Dec</td><td>TekGen, WebNLG</td><td>Subject hallucination, re-lation hallucination, ob-ject hallucination</td><td>Ontology driven KGC benchmark Text2KGBench</td></tr><tr><td>Li et al. (2023i)</td><td>Visual Question Answer</td><td>Enc-Dec</td><td>MSCOCO</td><td>Object hallucination</td><td>Caption hallucination assessment</td></tr></table>",
                                            "image_path": "2d07712010e0da5db6e1b37e5ade8b9143ba8f9b35157ff576e774177bc62de6.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 0,
                            "angle": 0,
                            "type": "table_body"
                        },
                        {
                            "bbox": [
                                205,
                                751,
                                387,
                                762
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        205,
                                        751,
                                        387,
                                        762
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                205,
                                                751,
                                                387,
                                                762
                                            ],
                                            "type": "text",
                                            "content": "Table 1: List of Representative Hallucination"
                                        }
                                    ]
                                }
                            ],
                            "index": 1,
                            "angle": 0,
                            "type": "table_caption"
                        }
                    ],
                    "index": 0
                }
            ],
            "discarded_blocks": [],
            "page_size": [
                595,
                841
            ],
            "page_idx": 4
        },
        {
            "para_blocks": [
                {
                    "type": "image",
                    "bbox": [
                        71,
                        68,
                        524,
                        200
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                71,
                                68,
                                524,
                                200
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        71,
                                        68,
                                        524,
                                        200
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                71,
                                                68,
                                                524,
                                                200
                                            ],
                                            "type": "image",
                                            "image_path": "86aab393efafee652a4a61a41284e26ffcd7f559f26b194349da9423c2323caf.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 0,
                            "angle": 0,
                            "type": "image_body"
                        },
                        {
                            "bbox": [
                                198,
                                206,
                                395,
                                219
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        198,
                                        206,
                                        395,
                                        219
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                198,
                                                206,
                                                395,
                                                219
                                            ],
                                            "type": "text",
                                            "content": "Figure 3: Taxonomy of Hallucination Detection."
                                        }
                                    ]
                                }
                            ],
                            "index": 1,
                            "angle": 0,
                            "type": "image_caption"
                        }
                    ],
                    "index": 0
                },
                {
                    "bbox": [
                        66,
                        241,
                        291,
                        403
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                66,
                                241,
                                291,
                                403
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        66,
                                        241,
                                        291,
                                        403
                                    ],
                                    "type": "text",
                                    "content": "of cross-modal tasks achieves promising progress (Zhu et al., 2023a; Liu et al., 2023b). However, despite replacing the original language encoder with LLMs, Large Visual Language Models (LVLMs) (Wang et al., 2022) still generate object descriptions that not present in the target image, denoted as object hallucinations (Li et al., 2023i). Especially, the various failure cases could be typically found in Visual Question Answering (Li et al., 2023i), Image Captioning (Biten et al., 2022; Petryk et al., 2023; Ning et al., 2023) and Report Generation (Mahmood et al., 2023) etc. cross-modal tasks."
                                }
                            ]
                        }
                    ],
                    "index": 2
                },
                {
                    "bbox": [
                        67,
                        413,
                        211,
                        426
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                67,
                                413,
                                211,
                                426
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        67,
                                        413,
                                        211,
                                        426
                                    ],
                                    "type": "text",
                                    "content": "4 Hallucination Detection"
                                }
                            ]
                        }
                    ],
                    "index": 3
                },
                {
                    "bbox": [
                        67,
                        434,
                        291,
                        718
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                67,
                                434,
                                291,
                                718
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        67,
                                        434,
                                        291,
                                        718
                                    ],
                                    "type": "text",
                                    "content": "Conventional hallucination detection mainly depends on task-specific metrics, such as ROUGE and BLEU to evaluate the information overlap between source and target texts in summarization tasks (Pagnoni et al., 2021), while knowledge F1 to estimate the knowledge-aware ability of response generation (Li et al., 2022). These metrics focus on measuring faithfulness of references and fail to provide an assessment of factualness. Despite some reference-free works are proposed, plugin-based methods (Dong et al., 2022) suffer from world knowledge limitation. QA-based matching metrics (Durmus et al., 2020) lack knowledge completeness of source information. NLI-based methods (Dziri et al., 2022c) are unable to support finer-grained hallucination checking as they are sentence-level, besides entailment and hallucination problems are not equivalent. As the paradigm shift in hallucination detection arising from the rapid development of LLMs, we present a novel taxonomy in Fig 3 and introduce each category in following sections."
                                }
                            ]
                        }
                    ],
                    "index": 4
                },
                {
                    "bbox": [
                        67,
                        719,
                        291,
                        773
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                67,
                                719,
                                291,
                                773
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        67,
                                        719,
                                        291,
                                        773
                                    ],
                                    "type": "text",
                                    "content": "- Inference Classifier. The most straightforward strategy involves adopting classifiers to assess the likelihood of hallucinations. Concretely, given a question "
                                },
                                {
                                    "bbox": [
                                        67,
                                        719,
                                        291,
                                        773
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathcal{Q}"
                                },
                                {
                                    "bbox": [
                                        67,
                                        719,
                                        291,
                                        773
                                    ],
                                    "type": "text",
                                    "content": " and an answer "
                                },
                                {
                                    "bbox": [
                                        67,
                                        719,
                                        291,
                                        773
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathcal{A}"
                                },
                                {
                                    "bbox": [
                                        67,
                                        719,
                                        291,
                                        773
                                    ],
                                    "type": "text",
                                    "content": ", an inferential classi"
                                }
                            ]
                        }
                    ],
                    "index": 5
                },
                {
                    "bbox": [
                        301,
                        241,
                        526,
                        512
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                301,
                                241,
                                526,
                                512
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        301,
                                        241,
                                        526,
                                        512
                                    ],
                                    "type": "text",
                                    "content": "fier "
                                },
                                {
                                    "bbox": [
                                        301,
                                        241,
                                        526,
                                        512
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathcal{C}"
                                },
                                {
                                    "bbox": [
                                        301,
                                        241,
                                        526,
                                        512
                                    ],
                                    "type": "text",
                                    "content": " can be asked to determine whether the answer contains hallucinatory content "
                                },
                                {
                                    "bbox": [
                                        301,
                                        241,
                                        526,
                                        512
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathcal{H}"
                                },
                                {
                                    "bbox": [
                                        301,
                                        241,
                                        526,
                                        512
                                    ],
                                    "type": "text",
                                    "content": " via computing "
                                },
                                {
                                    "bbox": [
                                        301,
                                        241,
                                        526,
                                        512
                                    ],
                                    "type": "inline_equation",
                                    "content": "p(\\mathcal{H}) = \\mathcal{F}_{\\mathcal{C}}(\\mathcal{Q},\\mathcal{A})"
                                },
                                {
                                    "bbox": [
                                        301,
                                        241,
                                        526,
                                        512
                                    ],
                                    "type": "text",
                                    "content": ". Therefore, Shen et al. (2023) employs the state-of-the-art LLMs to do end-to-end text generation of detection results. Some other studies (Li et al., 2023b) found that adding chains of thought precede output may intervene in the final judgement, whereas retrieving the knowledge properly resulted in gains. Furthering this concept, the hinted classifier and explainer (Shen et al., 2023), used to generate intermediate process labels and high-quality natural language explanations, were demonstrated to enhance the final predicted class from a variety of perspectives. Subsequently, Tam et al. (2023) suggests adopting a different classifier model to the generated model, contributing to easier judgement of factual consistency. For radiology report generation, binary classifiers (Mahmood et al., 2023) can be leveraged to measure the reliability by combining image and text embedding."
                                }
                            ]
                        }
                    ],
                    "index": 6
                },
                {
                    "bbox": [
                        302,
                        516,
                        526,
                        773
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                302,
                                516,
                                526,
                                773
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        302,
                                        516,
                                        526,
                                        773
                                    ],
                                    "type": "text",
                                    "content": "- Uncertainty Metric. It is important to examine the correlation between the hallucination metric and the quality of output from a variety of perspectives. One intuitive approach is to employ the probabilistic output of the model itself, as ASTSN (Varshney et al., 2023) calculates the models' uncertainty about the identified concepts by utilising the logit output values. Similarly, BARTSCORE (Yuan et al., 2021) employs a universal notion that models trained to convert generated text to reference output or source text will score higher when the generated text is superior. It is an unsupervised metric that supports the addition of appropriate prompts to improve the measure design, without human judgement to train. Furthermore, KoK (Amayuelas et al., 2023) based on the work of Pei and Jurgens (2021) evaluates answer uncertainty from three categories, i.e., subjectivity, hedges and text uncertainty. However, SLAG (Hase"
                                }
                            ]
                        }
                    ],
                    "index": 7
                }
            ],
            "discarded_blocks": [],
            "page_size": [
                595,
                841
            ],
            "page_idx": 5
        },
        {
            "para_blocks": [
                {
                    "type": "image",
                    "bbox": [
                        70,
                        68,
                        295,
                        238
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                70,
                                68,
                                295,
                                238
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        70,
                                        68,
                                        295,
                                        238
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                70,
                                                68,
                                                295,
                                                238
                                            ],
                                            "type": "image",
                                            "image_path": "b898f72efa27b9fe97cb83a0b4820b0f670c3e1a67b368b9676ce0582e7ffef1.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 0,
                            "angle": 0,
                            "type": "image_body"
                        },
                        {
                            "bbox": [
                                67,
                                245,
                                290,
                                270
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        67,
                                        245,
                                        290,
                                        270
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                67,
                                                245,
                                                290,
                                                270
                                            ],
                                            "type": "text",
                                            "content": "Figure 4: Sankey diagram of hallucination correction methods with different mainstream NLP tasks."
                                        }
                                    ]
                                }
                            ],
                            "index": 1,
                            "angle": 0,
                            "type": "image_caption"
                        }
                    ],
                    "index": 0
                },
                {
                    "bbox": [
                        67,
                        296,
                        291,
                        417
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                67,
                                296,
                                291,
                                417
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        67,
                                        296,
                                        291,
                                        417
                                    ],
                                    "type": "text",
                                    "content": "et al., 2023) measures consistent factual beliefs in terms of paraphrase, logic, and entailment. In addition to this, KLD (Pezeshkpour, 2023) combines information theory-based metrics (e.g., entropy and KL-divergence) to capture knowledge uncertainty. Beside expert-stipulated programmatic supervision, POLAR (Zhao et al., 2023b) introduces Pareto optimal learning assessed risk score for estimating the confidence level of a response."
                                }
                            ]
                        }
                    ],
                    "index": 2
                },
                {
                    "bbox": [
                        67,
                        421,
                        292,
                        775
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                67,
                                421,
                                292,
                                775
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        67,
                                        421,
                                        292,
                                        775
                                    ],
                                    "type": "text",
                                    "content": "- Self-Evaluation. To self-evaluate is challenging since the model might be overconfident about its generated samples being correct. The motivating idea of SelfCheckGPT (Manakul et al., 2023) is to use the ability of the LLMs themselves to sample multiple responses and identify fictitious statements by measuring the consistency of information among responses. Kadavath et al. (2022) further illustrates that both the increase in size and the demonstration of assessment can improve self-assessment. Beyond repetitive multiple direct queries, Agrawal et al. (2023) uses open-ended indirect queries and compares their answers to each other for an agreed-upon score outcome. Self-Contradictory (Mรผndler et al., 2023) imposes appropriate constraints on the same LLM to generate pairs of sentences triggering self-contradictions, which prompt the detection. In contrast, Polling-based querying (Li et al., 2023i) reduce the complexity of judgement by randomly sampling query objects. Besides, Self-Checker (Li et al., 2023d) decomposes complex statements into multiple simple statements, fact-checking them one by one. However, Cohen et al. (2023) introduces two LLMs interacting cross examination to drive the complex"
                                }
                            ]
                        }
                    ],
                    "index": 3
                },
                {
                    "bbox": [
                        302,
                        71,
                        449,
                        84
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                302,
                                71,
                                449,
                                84
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        302,
                                        71,
                                        449,
                                        84
                                    ],
                                    "type": "text",
                                    "content": "fact-checking reasoning process."
                                }
                            ]
                        }
                    ],
                    "index": 4
                },
                {
                    "bbox": [
                        302,
                        86,
                        527,
                        289
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                302,
                                86,
                                527,
                                289
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        302,
                                        86,
                                        527,
                                        289
                                    ],
                                    "type": "text",
                                    "content": "- Evidence Retrieval. Evidence retrieval accomplishes factual detection by retrieving supporting evidence related to hallucinations. To this end, Designing a claim-centric pipeline allows for a question-retrieve-summary chain to effectively collect original evidence (Chen et al., 2023b; Huo et al., 2023). Consequently, FActScore (Min et al., 2023) calculates the percentage of atomic facts supported by the given knowledge source. Towards adapting the tasks that users in interaction with generative models, FacTool (Chern et al., 2023) proposes to integrate a variety of tools into a task-agnostic and domain-agnostic detection framework, in order to assemble evidence about the authenticity of the generated content."
                                }
                            ]
                        }
                    ],
                    "index": 5
                },
                {
                    "bbox": [
                        302,
                        301,
                        453,
                        314
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                302,
                                301,
                                453,
                                314
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        302,
                                        301,
                                        453,
                                        314
                                    ],
                                    "type": "text",
                                    "content": "5 Hallucination Correction"
                                }
                            ]
                        }
                    ],
                    "index": 6
                },
                {
                    "bbox": [
                        302,
                        326,
                        527,
                        488
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                302,
                                326,
                                527,
                                488
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        302,
                                        326,
                                        527,
                                        488
                                    ],
                                    "type": "text",
                                    "content": "In this section, we delve into the methods to correct hallucination in terms of different aspects, i.e. Parameter Adaptation, Post-hoc Attribution and Edit Technology, Leverage External Knowledge, Assessment Feedback, Mindset Society. As shown in Figure 4, these hallucination correction paradigms have demonstrated strong dominance in many mainstream NLP tasks. Note that these methods are not entirely orthogonal but could complement each other as required by the tasks in practical applications. In the following sections, we will introduce each method as shown in Figure 5."
                                }
                            ]
                        }
                    ],
                    "index": 7
                },
                {
                    "bbox": [
                        302,
                        489,
                        527,
                        774
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                302,
                                489,
                                527,
                                774
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        302,
                                        489,
                                        527,
                                        774
                                    ],
                                    "type": "text",
                                    "content": "- Parameter Adaptation. Parameters in LLMs store biases learned in pre-training, are often unaligned with user intent. A cutting-edge strategy is to guide effective knowledge through parameter conditioning, editing, and optimisation. For example, CLR (Sun et al., 2023) optimises to reduce the generation probability of negative samples at span level utilising contrastive learning parameters. While introducing contextual knowledge background that contradicts the model's intrinsic prior knowledge, TYE (Shi et al., 2023a) effectively reduces the weight of prior knowledge through context-aware decoding method. Besides, PURR (Chen et al., 2023a) corrupts noise into the text, fine-tune compact editors, and denoise by merging relevant evidence. To introduce additional cache component, HISTALIGN (Wan et al., 2023) discovers that its hidden state is not aligned with the current hidden state, and proposes sequence information contrastive learning to improve the reliability of memory parameters. Consequently,"
                                }
                            ]
                        }
                    ],
                    "index": 8
                }
            ],
            "discarded_blocks": [],
            "page_size": [
                595,
                841
            ],
            "page_idx": 6
        },
        {
            "para_blocks": [
                {
                    "type": "image",
                    "bbox": [
                        71,
                        68,
                        530,
                        301
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                71,
                                68,
                                530,
                                301
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        71,
                                        68,
                                        530,
                                        301
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                71,
                                                68,
                                                530,
                                                301
                                            ],
                                            "type": "image",
                                            "image_path": "ed56c38d72f05c6fba15bbb6551b4e42851bc42abca721d68ca3eb45a6632fdd.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 0,
                            "angle": 0,
                            "type": "image_body"
                        },
                        {
                            "bbox": [
                                196,
                                309,
                                396,
                                321
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        196,
                                        309,
                                        396,
                                        321
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                196,
                                                309,
                                                396,
                                                321
                                            ],
                                            "type": "text",
                                            "content": "Figure 5: Taxonomy of Hallucination Correction."
                                        }
                                    ]
                                }
                            ],
                            "index": 1,
                            "angle": 0,
                            "type": "image_caption"
                        }
                    ],
                    "index": 0
                },
                {
                    "bbox": [
                        67,
                        343,
                        291,
                        532
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                67,
                                343,
                                291,
                                532
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        67,
                                        343,
                                        291,
                                        532
                                    ],
                                    "type": "text",
                                    "content": "Edit-TA (Ilharco et al., 2023) mitigates the biases learnt in pre-training from a task algorithm perspective. An intuition behind it is that parameter variations learnt through negative example tasks could be perceived through weight variances. However as this fails to take the importance of different negative examples into account, therefore EwR (Daheim et al., 2023) proposes Fisher information matrices to measure the uncertainty of their estimation, which is applied for the dialogue systems to execute a parameter interpolation and remove hallucination. EasyEdit (Wang et al., 2023c) summarises methods for parameter editing, while minimising the influence to irrelevant parameter."
                                }
                            ]
                        }
                    ],
                    "index": 2
                },
                {
                    "bbox": [
                        67,
                        543,
                        291,
                        773
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                67,
                                543,
                                291,
                                773
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        67,
                                        543,
                                        291,
                                        773
                                    ],
                                    "type": "text",
                                    "content": "An efficient alternative is to identify task-specific parameters and exploit them. For example, ALLM (Luo et al., 2023) aligns the parameter module with task-specific knowledge, and then generates the relevant knowledge as additional context in background augmented prompts. Similarly, mmT5 (Pfeiffer et al., 2023) utilises language-specific modules during pre-training to separate language-specific information from language-independent information, demonstrating that adding language-specific modules can alleviate the curse of multilinguality. Instead, TRAC (Li et al., 2023f) combines conformal prediction and global testing to augment retrieval-based QA. The conservative strategy formulation ensures that a semantically equivalent answer to the truthful answer is included in the prediction set."
                                }
                            ]
                        }
                    ],
                    "index": 3
                },
                {
                    "bbox": [
                        302,
                        343,
                        526,
                        519
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                302,
                                343,
                                526,
                                519
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        302,
                                        343,
                                        526,
                                        519
                                    ],
                                    "type": "text",
                                    "content": "Another parameter adaptation idea focuses on flexible sampling consistent with user requirements. For instance, Lee et al. (2022) observes that the randomness of sampling is more detrimental to factuality when generating the latter part of a sentence. The factual-nucleus sampling algorithm is introduced to keep the faithfulness of the generation while ensuring the quality and diversity. Besides, Inference-Time (Li et al., 2023c) firstly identifies a set of attentional heads with high linear probing accuracy, and then shifts activation in the inference process along the direction associated with factual knowledge."
                                }
                            ]
                        }
                    ],
                    "index": 4
                },
                {
                    "bbox": [
                        302,
                        529,
                        526,
                        773
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                302,
                                529,
                                526,
                                773
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        302,
                                        529,
                                        526,
                                        773
                                    ],
                                    "type": "text",
                                    "content": "- Post-hoc Attribution and Edit Technology. A source of hallucination is that LLMs may leverage the patterns observed in the pre-training data for inference in a novel form. Recently, ORCA (Han and Tsvetkov, 2022) reveals problematic patterns in the behaviour of models by probing supporting data evidences from pre-training data. Similarly, TRAK (Park et al., 2023) and Data-Portraits (Marone and Durme, 2023) analyse whether models plagiarise or reference existing resources by means of data attribution. QUIP (Weller et al., 2023) further demonstrates that providing text that has been observed in the pre-training phase can improve the ability of LLMs to generate more factual information. Furthermore, motivated by the gap between LLMs and human modes of thinking, one intuition is to align the two modes of reasoning. Thus CoT (Wei et al., 2022b) elicits"
                                }
                            ]
                        }
                    ],
                    "index": 5
                }
            ],
            "discarded_blocks": [],
            "page_size": [
                595,
                841
            ],
            "page_idx": 7
        },
        {
            "para_blocks": [
                {
                    "bbox": [
                        69,
                        71,
                        291,
                        368
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                69,
                                71,
                                291,
                                368
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        69,
                                        71,
                                        291,
                                        368
                                    ],
                                    "type": "text",
                                    "content": "faithful reasoning via a kind of Chain-of-Thought (CoT) (Kojima et al., 2022) prompts. Similarly, RR (He et al., 2023) retrieves relevant external knowledge based on decomposed reasoning steps obtained from a CoT prompt. Since LLMs not often produce the best output on the first attempt, Self-Refine (Madaan et al., 2023) implements self-refinement algorithms through iterative feedback and improvement. Reflexion (Shinn et al., 2023) also employs verbal reinforcement to generate reflective feedback by learning about prior failings. Verify-and-Edit (Zhao et al., 2023a) proposes a CoT-prompted verify-and-edit framework, which improves the fidelity of predictions by post-editing the inference chain based on externally retrieved knowledge. Another source of hallucinations is to describe factual content with incorrect retrievals. To recy this, NP-Hunter (Dziri et al., 2021) follows a generate-then-refine strategy whereby a generated response is amended using the KG so that the dialogue systemable to correct potential hallucinations by querying the KG."
                                }
                            ]
                        }
                    ],
                    "index": 0
                },
                {
                    "bbox": [
                        69,
                        380,
                        291,
                        772
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                69,
                                380,
                                291,
                                772
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        69,
                                        380,
                                        291,
                                        772
                                    ],
                                    "type": "text",
                                    "content": "- Leverage External Knowledge. As an attempt to extend the language model for halocination mitigation, a suggestion is to retrieve relevant documents from large textual databases. RETRO (Borgeaud et al., 2022) splits the input sequence into chunks and retrieves similar documents, while In-Context RALM (Tam et al., 2023) places the selected document before the input text to improve the prediction. Furthermore, IRCOT (Trivedi et al., 2023) interweaves CoT generation and document retrieval steps to guide LLMs. Since scaling mainly improves the memory for common knowledge but does not significantly improve the memory for factual knowledge in the long tail, POPQA (Mallen et al., 2023) retrieves only non-parametric memories when necessary to improve performance. LLM-AUGENTER (Peng et al., 2023) also bases the responses of LLMs on integrated external knowledge and automated feedback to improve the truthfulness score of the answers. Another work, CoK (Li et al., 2023h) iteratively analyses future content of upcoming sentences, and then applies them as a query to retrieve relevant documents for the purposes of re-generating sentences when they contain low confidence tokens. Similarly, RETA-LLM (Liu et al., 2023c) creates a complete pipeline to assist users in building their own domain-based LLM retrieval systems. Note that in addition to document re"
                                }
                            ]
                        }
                    ],
                    "index": 1
                },
                {
                    "bbox": [
                        305,
                        71,
                        525,
                        314
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                305,
                                71,
                                525,
                                314
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        305,
                                        71,
                                        525,
                                        314
                                    ],
                                    "type": "text",
                                    "content": "trieval, diverse external knowledge queries could be assembled into retrieval-augmented LLM systems. For example, FLARE (Jiang et al., 2023) leverages structured knowledge bases to support complex queries and provide more straightforward factual statements. Further, KnowledGPT (Wang et al., 2023e) adopts program of thoughts (PoT) prompting, which generates codes to interact with knowledge bases. While cTBL (Ding et al., 2023) proposes to enhance LLMs with tabular data in conversation settings. Besides, GeneGPT (Jin et al., 2023) demonstrates that expertise can be accessed more easily and accurately by detecting and executing API calls through contextual learning and augmented decoding algorithms. To support potentially millions of ever-changing APIs, Gorilla (Patil et al., 2023) explores self-instruct fine-tuning and retrieval for efficient API exploitation."
                                }
                            ]
                        }
                    ],
                    "index": 2
                },
                {
                    "bbox": [
                        305,
                        321,
                        525,
                        631
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                305,
                                321,
                                525,
                                631
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        305,
                                        321,
                                        525,
                                        631
                                    ],
                                    "type": "text",
                                    "content": "- Assessment Feedback. As language models become more sophisticated, evaluation feedback can significantly improve the quality of generated text, as well as reduce the appearance of hallucinations. To realise this concept, LSHF (Stiennon et al., 2020) predicts human-preferred summarizations through model and employs this as the reward function to fine-tune the summarization strategy using reinforcement learning. However, this approach builds on models crafted by human annotators, which makes them inefficient in terms of data utilization. Therefore, TLM (Menick et al., 2022) proposes to improve the reliability of the system by selecting a few questions to refuse to answer, which significantly improves the reliability of the system, through reinforcement learning from human preferences. Whereas reinforcement learning often suffers from imperfect reward functions and relies on challenging optimisation. Thus, Chain-of-Hindsight (Liu et al., 2023a) converts feedback preferences into sentences, which are then fed into models with fine-tuning for enhanced language comprehension."
                                }
                            ]
                        }
                    ],
                    "index": 3
                },
                {
                    "bbox": [
                        305,
                        638,
                        525,
                        772
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                305,
                                638,
                                525,
                                772
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        305,
                                        638,
                                        525,
                                        772
                                    ],
                                    "type": "text",
                                    "content": "In addition to enabling the model to learn directly from the feedback of factual metrics in a sample-efficient manner (Dixit et al., 2023), it is also important to build in a self-evaluation function of the model to filter candidate generated texts. For example, BRIO (Liu et al., 2022) empowers summarization model assessment, estimating probability distributions of candidate outputs to rate the quality of candidate summaries. While LM-know (Kadavath et al., 2022) is devoted to in"
                                }
                            ]
                        }
                    ],
                    "index": 4
                }
            ],
            "discarded_blocks": [],
            "page_size": [
                595,
                841
            ],
            "page_idx": 8
        },
        {
            "para_blocks": [
                {
                    "bbox": [
                        69,
                        71,
                        289,
                        219
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                69,
                                71,
                                289,
                                219
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        69,
                                        71,
                                        289,
                                        219
                                    ],
                                    "type": "text",
                                    "content": "vestigating whether LLMs can evaluate the validity of their own claims by detecting the probability that they know the answer to a question. Consequently, Do-LLM-Know (Agrawal et al., 2023) queries exclusively with black-box LLMs, and the results of queries repeatedly generated multiple times are compared with each other to pass consistency checks. Besides, the black-box LLM is augmented with a plug-and-play retrieval module (Liu et al., 2023a; Huang et al., 2023) to generate feedback could improve the model response."
                                }
                            ]
                        }
                    ],
                    "index": 0
                },
                {
                    "bbox": [
                        69,
                        226,
                        289,
                        496
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                69,
                                226,
                                289,
                                496
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        69,
                                        226,
                                        289,
                                        496
                                    ],
                                    "type": "text",
                                    "content": "As missing citation quality evaluation affects the final performance, ALCE (Gao et al., 2023c) employs a natural language reasoning model to measure citation quality and extends the integrated retrieval system. Similarly, CRITIC (Gou et al., 2023) proposes to interact with appropriate tools to assess certain aspects of the text, and then to modify the output based on the feedback obtained during the verification process. Note that automated error checking can also utilise LLMs to generate text that conforms to tool interfaces. PaD (Zhu et al., 2023c) distills the LLMs with a synthetic inference procedure, and the synthesis program obtained can be automatically compiled and executed by an explainer. Further, iterative refinement processes are validated to effectively identify appropriate details (Ning et al., 2023; Zhang et al., 2023b; Yu et al., 2023b), and can stop early invalid reasoning chains, beneficially reducing the phenomenon of hallucination snowballing (Zhang et al., 2023a)."
                                }
                            ]
                        }
                    ],
                    "index": 1
                },
                {
                    "bbox": [
                        69,
                        502,
                        289,
                        772
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                69,
                                502,
                                289,
                                772
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        69,
                                        502,
                                        289,
                                        772
                                    ],
                                    "type": "text",
                                    "content": "- Mindset Society. Human intelligence thrives on the concept of cognitive synergy, where collaboration between different cognitive processes produces better results than isolated individual cognitive processes. \"Society of minds\" (Minsky, 1988) is believed have the potential to significantly improve the performance of LLMs and pave the way for consistency in language production and comprehension. For the purpose of addressing hallucinations in large-scale multilingual models across different translation scenarios, HLMTM (Guerreiro et al., 2023a) proposes a hybrid setting in which other translation systems can be requested to act as a back-up system when the original system is hallucinating. Consequently, Multiagent-Debate (Du et al., 2023) employs multiple LLMs in several rounds to propose and debate their individual responses and reasoning processes to reach a consensus final answer. As a result of this process, the models are encouraged to construct an"
                                }
                            ]
                        }
                    ],
                    "index": 2
                },
                {
                    "bbox": [
                        305,
                        71,
                        524,
                        394
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                305,
                                71,
                                524,
                                394
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        305,
                                        71,
                                        524,
                                        394
                                    ],
                                    "type": "text",
                                    "content": "svers that are consistent with both internal criticisms and responses from other agents. Before a final answer is presented, the resultant community of models can hold and maintain multiple reasoning chains and possible answers simultaneously. Based on this idea, MAD (Liang et al., 2023) adds a judge-managed debate process, demonstrating that adaptive interruptions of debate and controlled \"tit-for-tat\" states help to complete factual debates. Furthermore, FORD (Xiong et al., 2023) proposes roundtable debates that include more than two LLMs and emphasises that competent judges are essential to dominate the debate. LM-vs-LM (Cohen et al., 2023) also proposes multi-round interactions between LM and another LM to check the factualness of original statements. Besides, PRD (Li et al., 2023e) proposes a peer rank and discussionbased evaluation framework to arrive at a well-recognised assessment result that all peers are in agreement with. In an effort to maintain strong reasoning, SPP (Wang et al., 2023g) utilises LLMs to assign several fine-grained roles, which effectively stimulates knowledge acquisition and reduces hallucinations."
                                }
                            ]
                        }
                    ],
                    "index": 3
                },
                {
                    "bbox": [
                        305,
                        409,
                        413,
                        423
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                305,
                                409,
                                413,
                                423
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        305,
                                        409,
                                        413,
                                        423
                                    ],
                                    "type": "text",
                                    "content": "6 Future Directions"
                                }
                            ]
                        }
                    ],
                    "index": 4
                },
                {
                    "bbox": [
                        305,
                        434,
                        524,
                        473
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                305,
                                434,
                                524,
                                473
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        305,
                                        434,
                                        524,
                                        473
                                    ],
                                    "type": "text",
                                    "content": "Though numerous technical solutions have been proposed in the survey for hallucinations in LLMs, there exist some potential directions:"
                                }
                            ]
                        }
                    ],
                    "index": 5
                },
                {
                    "bbox": [
                        305,
                        476,
                        524,
                        772
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                305,
                                476,
                                524,
                                772
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        305,
                                        476,
                                        524,
                                        772
                                    ],
                                    "type": "text",
                                    "content": "- Data Construction Management. As previously discussed, the style, and knowledge of LLMs is basically learned during model pre-training. High quality data present promising opportunities for facilitating the reduction of hallucinations in LLMs (Kirstain et al., 2022). Inspired by the basic rule of machine learning models: \"Garbage input, garbage output\", Zhou et al. (2023) proposes the superficial alignment hypothesis, which views alignment as learning to interact with the user. The results of simple fine-tuning on a few high-quality samples demonstrate that data quality and diversity outweigh the importance of fine-tuning large-scale instructions (Mishra et al., 2021; Wei et al., 2022a; Sanh et al., 2022) and RLHF (Bai et al., 2022; Ouyang et al., 2022). To perform efficiently in knowledge-intensive verticals, we argue that construction of entity-centred fine-tuned instructions (Bao et al., 2023; Gui et al., 2023; Wei Zhu and Wang, 2023) is a promising direction that it can combine the structured knowledge and semantic relevance of knowledge graphs to enhance the factual"
                                }
                            ]
                        }
                    ],
                    "index": 6
                }
            ],
            "discarded_blocks": [],
            "page_size": [
                595,
                841
            ],
            "page_idx": 9
        },
        {
            "para_blocks": [
                {
                    "bbox": [
                        67,
                        71,
                        291,
                        166
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                67,
                                71,
                                291,
                                166
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        67,
                                        71,
                                        291,
                                        166
                                    ],
                                    "type": "text",
                                    "content": "ity of generated entity information. Another feasible proposal is to incorporate a self-curation phase (Li et al., 2023g) in the instruction construction process to rate the quality of candidate pairs. During the iteration process, quality evaluation (Chen et al., 2023c) based on manual or automated rule constraints could provide self-correction capacity."
                                }
                            ]
                        }
                    ],
                    "index": 0
                },
                {
                    "bbox": [
                        67,
                        169,
                        292,
                        645
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                67,
                                169,
                                292,
                                645
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        67,
                                        169,
                                        292,
                                        645
                                    ],
                                    "type": "text",
                                    "content": "- Downstream Task Alignment. Generic LLMs have a certain degree of natural language problem comprehension in a variety of open environments. However, the main problem still remains in the deviation from the application requirements, which leads to emergence of diverse hallucinations. Thus, downstream task alignment especially built on vertical domain cognition necessitates expanded symbolic reasoning, decomposition and planning of complex tasks, and faithful external knowledge injection. Specifically, while expert in language processing, LLMs struggle to make breakthroughs in mathematical abilities, a deficiency attributable to the textual training objective. Though some researches for symbolic math word problems (Gaur and Saunshi, 2023; Zhu et al., 2023b) have been proposed, enhancing symbolic reasoning and answering numerical questions remains to be explored extensively. Additionally, for story generation tasks that demand diverse outputs (Yang et al., 2022, 2023), fascinating storylines are required in addition to avoiding factual contradictions. Therefore, achieving a balance between faithfulness and creativity in the model inference process remains a crucial challenge. Moreover, integrating new knowledge to deal with knowledge-intensive tasks involves handling joint reasoning between the implicit knowledge of LLMs and the explicit knowledge of external knowledge graphs. There arises a challenge to design knowledge-aware methods to incorporate structured information from the knowledge graph into the pre-training process of LLMs. Alternatively, the reasoning process is expected to be dynamically injected with knowledge graph information (Wen et al., 2023)."
                                }
                            ]
                        }
                    ],
                    "index": 1
                },
                {
                    "bbox": [
                        67,
                        651,
                        292,
                        774
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                67,
                                651,
                                292,
                                774
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        67,
                                        651,
                                        292,
                                        774
                                    ],
                                    "type": "text",
                                    "content": "The utilization of LLMs as an evaluation tool is a burgeoning application, but limited by the size of models, the effect of instruction adjustments, and the different forms of inputs (Agrawal et al., 2023). Note attempts of LLMs to act as judges for scoring have to overcome all kinds of biases induced by position, morbidity, self-enhancement (Zheng et al., 2023a; Berglund et al., 2023; Wang et al., 2023b). Therefore, we forecast that future research about"
                                }
                            ]
                        }
                    ],
                    "index": 2
                },
                {
                    "bbox": [
                        302,
                        71,
                        526,
                        113
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                302,
                                71,
                                526,
                                113
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        302,
                                        71,
                                        526,
                                        113
                                    ],
                                    "type": "text",
                                    "content": "designing task-specific mechanisms for analysing and correcting processes of emerging downstream tasks is an area deserving long-term attention."
                                }
                            ]
                        }
                    ],
                    "index": 3
                },
                {
                    "bbox": [
                        302,
                        117,
                        527,
                        702
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                302,
                                117,
                                527,
                                702
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        302,
                                        117,
                                        527,
                                        702
                                    ],
                                    "type": "text",
                                    "content": "- Reasoning Mechanism Exploitation. The emerging CoT technique (Wei et al., 2022b) stimulates the emergent reasoning ability of LLMs by imitating intrinsic stream of thought. Constructing a logically intermediate reasoning step has been proved to significantly improve the problem-solving ability. Recently, A primary improvement is Self-consistency with CoT (CoT-SC) (Yao et al., 2023), which is a method for generating multiple CoT options and then selecting the optimal result as feedback. Further, Tree of Thoughts (ToT) (Yao et al., 2023) introduces a strict tree architecture into the thought process, which facilitates the development with different paths of thought and provides a novel roll-back function. Since previous methods have no storages for intermediate results, Cumulative Reasoning (CR) (Zhang et al., 2023c) uses LLMs in a cumulative and iterative manner to simulate human thought processes, and decompose the task into smaller components. However, the actual thinking process creates a complex network of ideas, as an example, people could explore a particular chain of reasoning, backtrack or start a new chain of reasoning. In particular when aware that an idea from a previous chain of reasoning can be combined with the currently explored idea, they could be merged into a new solution. More excitingly, Graph of Thoughts(GoT) (Zhang et al., 2023c) extends the dependencies between thoughts by constructing vertices with multiple incoming edges to aggregate arbitrary thoughts. In addition, Program-aided language models (PAL) (Gao et al., 2023b) and Program of Thoughts prompting (PoT) (Chen et al., 2022b) introduce programming logic into the language space (Bi et al., 2023), expanding the ability to invoke external explainers. As a summary, we believe that research based on human cognition helps to provide brilliant and insightful insights into the analysis of hallucinations, such as Dual Process Theory (Frankish, 2010), Three layer mental model (Stanovich, 2011), Computational Theory of Mind (Piccinini, 2004), and Connectionism (Thorndike, 1898)."
                                }
                            ]
                        }
                    ],
                    "index": 4
                },
                {
                    "bbox": [
                        302,
                        705,
                        527,
                        775
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                302,
                                705,
                                527,
                                775
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        302,
                                        705,
                                        527,
                                        775
                                    ],
                                    "type": "text",
                                    "content": "- Multi-modal Hallucination Survey. It has become a community consensus to establish powerful Multimodal Large Language Models (MLLMs) (Li et al., 2023a; Dai et al., 2023; Ye et al., 2023) by taking advantage of excellent comprehension and"
                                }
                            ]
                        }
                    ],
                    "index": 5
                }
            ],
            "discarded_blocks": [],
            "page_size": [
                595,
                841
            ],
            "page_idx": 10
        },
        {
            "para_blocks": [
                {
                    "bbox": [
                        69,
                        71,
                        291,
                        558
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                69,
                                71,
                                291,
                                558
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        69,
                                        71,
                                        291,
                                        558
                                    ],
                                    "type": "text",
                                    "content": "reasoning capabilities of LLMs. Li et al. (2023i) confirms the severity of hallucinations in MLLM by object detecting and polling-based querying. The results indicate that the models are highly susceptible to object hallucination, and the generated description does not match the target image. Besides, Shao et al. (2023) that MLLMs have limited multimodal reasoning ability as well as dependence on spurious cues. Though current study (Yin et al., 2023b) provides a broad overview of MLLMs, the causation of hallucinations has not been comprehensively investigated. The hallucinations in LLMs come mainly from misknowledge in the training data, whereas the challenge of MLLMs lies in accurately relaying the abstract visual encoding into the semantic space. Existing MLLMs are fine-tuned with instructions to make their target outputs follow human intentions. However, misalignment between visual and textual modes may lead to biased distribution. Further, the lack of visual constraints results in a serious problem of hallucination in MLLMs. Thus a potential improvement is to penalise deviating attention to images (Wang et al., 2023a) or to enhance understanding of visual common sense. In terms of fine-grained visual and textual modal alignment, focusing on local features of images and corresponding textual descriptions can provide faithful modal interactions. In addition, the performance of some MLLMs such as MiniGPT-4 (Zhu et al., 2023a) is highly dependent on the choice of prompts and requires careful selection. Note that a controlled trade-off between diversity and hallucinations is needed for user convenience. In the future, as more sophisticated multi-model applications emerge, improving MLLMs reasoning paths is also a promising research direction."
                                }
                            ]
                        }
                    ],
                    "index": 0
                },
                {
                    "bbox": [
                        69,
                        573,
                        202,
                        585
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                69,
                                573,
                                202,
                                585
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        69,
                                        573,
                                        202,
                                        585
                                    ],
                                    "type": "text",
                                    "content": "7 Conclusion and Vision"
                                }
                            ]
                        }
                    ],
                    "index": 1
                },
                {
                    "bbox": [
                        69,
                        597,
                        291,
                        772
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                69,
                                597,
                                291,
                                772
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        69,
                                        597,
                                        291,
                                        772
                                    ],
                                    "type": "text",
                                    "content": "In this paper, we provide an overview of hallucinations in LLMs with new taxonomy, theoretical insight, detection methods, correction methods and several future research directions. Note that it is crucial to ensure we can continuously utilize LLMs in a responsible and beneficial manner, so we explore the causation of hallucinations and taxonomy in task axes to analyse potential directions for improvement. In the future, we envision a more potent synergy between LLMs and external knowledge bases, resulting in a credible interactive system with the dual-wheel drive. We hope that sophisticated and efficient detection methods are proposed"
                                }
                            ]
                        }
                    ],
                    "index": 2
                },
                {
                    "bbox": [
                        305,
                        71,
                        524,
                        165
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                305,
                                71,
                                524,
                                165
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        305,
                                        71,
                                        524,
                                        165
                                    ],
                                    "type": "text",
                                    "content": "to contribute further to improving the performance of LLMs. Furthermore, we hope that community maintains a proactive attitude towards mitigating the effects of hallucinations. With creative corrective methods proposed for various aspects, LLMs will provide human with reliable and secure information in broad application scenarios."
                                }
                            ]
                        }
                    ],
                    "index": 3
                },
                {
                    "bbox": [
                        305,
                        189,
                        361,
                        200
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                305,
                                189,
                                361,
                                200
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        305,
                                        189,
                                        361,
                                        200
                                    ],
                                    "type": "text",
                                    "content": "References"
                                }
                            ]
                        }
                    ],
                    "index": 4
                },
                {
                    "bbox": [
                        304,
                        207,
                        526,
                        772
                    ],
                    "type": "list",
                    "angle": 0,
                    "index": 14,
                    "blocks": [
                        {
                            "bbox": [
                                305,
                                207,
                                525,
                                262
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        305,
                                        207,
                                        525,
                                        262
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                305,
                                                207,
                                                525,
                                                262
                                            ],
                                            "type": "text",
                                            "content": "Vaibhav Adlakha, Parishad BehnamGhader, Xing Han Lu, Nicholas Meade, and Siva Reddy. 2023. Evaluating correctness and faithfulness of instruction-following models for question answering. CoRR, abs/2307.16877."
                                        }
                                    ]
                                }
                            ],
                            "index": 5
                        },
                        {
                            "bbox": [
                                305,
                                271,
                                525,
                                305
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        305,
                                        271,
                                        525,
                                        305
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                305,
                                                271,
                                                525,
                                                305
                                            ],
                                            "type": "text",
                                            "content": "Ayush Agrawal, Lester Mackey, and Adam Tauman Kalai. 2023. Do language models know when they're hallucinating references? CoRR, abs/2305.18248."
                                        }
                                    ]
                                }
                            ],
                            "index": 6
                        },
                        {
                            "bbox": [
                                304,
                                312,
                                525,
                                357
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        312,
                                        525,
                                        357
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                312,
                                                525,
                                                357
                                            ],
                                            "type": "text",
                                            "content": "Alfonso Amayuelas, Liangming Pan, Wenhu Chen, and William Yang Wang. 2023. Knowledge of knowledge: Exploring known-unknowns uncertainty with large language models. CoRR, abs/2305.13712."
                                        }
                                    ]
                                }
                            ],
                            "index": 7
                        },
                        {
                            "bbox": [
                                304,
                                365,
                                526,
                                508
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        365,
                                        526,
                                        508
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                365,
                                                526,
                                                508
                                            ],
                                            "type": "text",
                                            "content": "Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Chris Olah, Benjamin Mann, and Jared Kaplan. 2022. Training a helpful and harmless assistant with reinforcement learning from human feedback. CoRR, abs/2204.05862."
                                        }
                                    ]
                                }
                            ],
                            "index": 8
                        },
                        {
                            "bbox": [
                                304,
                                517,
                                525,
                                571
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        517,
                                        525,
                                        571
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                517,
                                                525,
                                                571
                                            ],
                                            "type": "text",
                                            "content": "Zhijie Bao, Wei Chen, Shengze Xiao, Kuang Ren, Jiao Wu, Cheng Zhong, Jiajie Peng, Xuanjing Huang, and Zhongyu Wei. 2023. Disc-medllm: Bridging general large language models and real-world medical consultation."
                                        }
                                    ]
                                }
                            ],
                            "index": 9
                        },
                        {
                            "bbox": [
                                304,
                                580,
                                525,
                                624
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        580,
                                        525,
                                        624
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                580,
                                                525,
                                                624
                                            ],
                                            "type": "text",
                                            "content": "Rachel Bawden and Franรงois Yvon. 2023. Investigating the translation performance of a large multilingual language model: the case of BLOOM. CoRR, abs/2303.01911."
                                        }
                                    ]
                                }
                            ],
                            "index": 10
                        },
                        {
                            "bbox": [
                                304,
                                633,
                                525,
                                678
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        633,
                                        525,
                                        678
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                633,
                                                525,
                                                678
                                            ],
                                            "type": "text",
                                            "content": "Lukas Berglund, Asa Cooper Stickland, Mikita Balesni, Max Kaufmann, Meg Tong, Tomasz Korbak, Daniel Kokotajlo, and Owain Evans. 2023. Taken out of context: On measuring situational awareness in llms."
                                        }
                                    ]
                                }
                            ],
                            "index": 11
                        },
                        {
                            "bbox": [
                                304,
                                686,
                                525,
                                719
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        686,
                                        525,
                                        719
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                686,
                                                525,
                                                719
                                            ],
                                            "type": "text",
                                            "content": "Zhen Bi, Ningyu Zhang, Yinuo Jiang, Shumin Deng, Guozhou Zheng, and Huajun Chen. 2023. When do program-of-thoughts work for reasoning?"
                                        }
                                    ]
                                }
                            ],
                            "index": 12
                        },
                        {
                            "bbox": [
                                304,
                                728,
                                525,
                                772
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        728,
                                        525,
                                        772
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                728,
                                                525,
                                                772
                                            ],
                                            "type": "text",
                                            "content": "Ali Furkan Biten, Lluรญs Gรณmez, and Dimosthenis Karatzas. 2022. Let there be a clock on the beach: Reducing object hallucination in image captioning. In IEEE/CVF Winter Conference on Applications of"
                                        }
                                    ]
                                }
                            ],
                            "index": 13
                        }
                    ],
                    "sub_type": "ref_text"
                }
            ],
            "discarded_blocks": [],
            "page_size": [
                595,
                841
            ],
            "page_idx": 11
        },
        {
            "para_blocks": [
                {
                    "bbox": [
                        69,
                        72,
                        291,
                        772
                    ],
                    "type": "list",
                    "angle": 0,
                    "index": 8,
                    "blocks": [
                        {
                            "bbox": [
                                80,
                                72,
                                290,
                                95
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        80,
                                        72,
                                        290,
                                        95
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                80,
                                                72,
                                                290,
                                                95
                                            ],
                                            "type": "text",
                                            "content": "Computer Vision, WACV 2022, Waikoloa, HI, USA, January 3-8, 2022, pages 2473-2482. IEEE."
                                        }
                                    ]
                                }
                            ],
                            "index": 0
                        },
                        {
                            "bbox": [
                                69,
                                103,
                                291,
                                267
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        103,
                                        291,
                                        267
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                103,
                                                291,
                                                267
                                            ],
                                            "type": "text",
                                            "content": "Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. 2022. Improving language models by retrieving from trillions of tokens. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 2206-2240. PMLR."
                                        }
                                    ]
                                }
                            ],
                            "index": 1
                        },
                        {
                            "bbox": [
                                69,
                                276,
                                291,
                                441
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        276,
                                        291,
                                        441
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                276,
                                                291,
                                                441
                                            ],
                                            "type": "text",
                                            "content": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual."
                                        }
                                    ]
                                }
                            ],
                            "index": 2
                        },
                        {
                            "bbox": [
                                69,
                                449,
                                291,
                                539
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        449,
                                        291,
                                        539
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                449,
                                                291,
                                                539
                                            ],
                                            "type": "text",
                                            "content": "Meng Cao, Yue Dong, and Jackie Chi Kit Cheung. 2022. Hallucinated but factual! inspecting the factuality of hallucinations in abstractive summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 3340-3354. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 3
                        },
                        {
                            "bbox": [
                                69,
                                547,
                                290,
                                602
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        547,
                                        290,
                                        602
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                547,
                                                290,
                                                602
                                            ],
                                            "type": "text",
                                            "content": "Stephanie Chan, Adam Santoro, Andrew K. Lampinen, Jane Wang, Aaditya Singh, Pierre H. Richemond, James L. McClelland, and Felix Hill. 2022. Data distributional properties drive emergent in-context learning in transformers. In NeurIPS."
                                        }
                                    ]
                                }
                            ],
                            "index": 4
                        },
                        {
                            "bbox": [
                                69,
                                611,
                                291,
                                655
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        611,
                                        291,
                                        655
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                611,
                                                291,
                                                655
                                            ],
                                            "type": "text",
                                            "content": "Anthony Chen, Panupong Pasupat, Sameer Singh, Hongrae Lee, and Kelvin Guu. 2023a. PURR: efficiently editing language model hallucinations by denoising language model corruptions. CoRR, abs/2305.14908."
                                        }
                                    ]
                                }
                            ],
                            "index": 5
                        },
                        {
                            "bbox": [
                                69,
                                664,
                                291,
                                708
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        664,
                                        291,
                                        708
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                664,
                                                291,
                                                708
                                            ],
                                            "type": "text",
                                            "content": "Jifan Chen, Grace Kim, Aniruddh Sriram, Greg Durrett, and Eunsol Choi. 2023b. Complex claim verification with evidence retrieved in the wild. CoRR, abs/2305.11859."
                                        }
                                    ]
                                }
                            ],
                            "index": 6
                        },
                        {
                            "bbox": [
                                69,
                                716,
                                291,
                                772
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        716,
                                        291,
                                        772
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                716,
                                                291,
                                                772
                                            ],
                                            "type": "text",
                                            "content": "Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxia Jin. 2023c. Alpagasus: Training A better alpaca with fewer data. CoRR, abs/2307.08701."
                                        }
                                    ]
                                }
                            ],
                            "index": 7
                        }
                    ],
                    "sub_type": "ref_text"
                },
                {
                    "bbox": [
                        304,
                        72,
                        526,
                        772
                    ],
                    "type": "list",
                    "angle": 0,
                    "index": 16,
                    "blocks": [
                        {
                            "bbox": [
                                304,
                                72,
                                526,
                                182
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        72,
                                        526,
                                        182
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                72,
                                                526,
                                                182
                                            ],
                                            "type": "text",
                                            "content": "Mingda Chen, Jingfei Du, Ramakanth Pasunuru, Todor Mihaylov, Srini Iyer, Veselin Stoyanov, and Zornitsa Kozareva. 2022a. Improving in-context few-shot learning via self-supervised training. In Proceedings of the 2022 Conference of the North American Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022, pages 3558-3573. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 9
                        },
                        {
                            "bbox": [
                                304,
                                192,
                                526,
                                247
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        192,
                                        526,
                                        247
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                192,
                                                526,
                                                247
                                            ],
                                            "type": "text",
                                            "content": "Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. 2022b. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. CoRR, abs/2211.12588."
                                        }
                                    ]
                                }
                            ],
                            "index": 10
                        },
                        {
                            "bbox": [
                                304,
                                258,
                                526,
                                323
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        258,
                                        526,
                                        323
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                258,
                                                526,
                                                323
                                            ],
                                            "type": "text",
                                            "content": "I-Chun Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, and Pengfei Liu. 2023. Factool: Factuality detection in generative AI - A tool augmented framework for multi-task and multi-domain scenarios. CoRR, abs/2307.13528."
                                        }
                                    ]
                                }
                            ],
                            "index": 11
                        },
                        {
                            "bbox": [
                                304,
                                334,
                                526,
                                433
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        334,
                                        526,
                                        433
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                334,
                                                526,
                                                433
                                            ],
                                            "type": "text",
                                            "content": "Sabrina Chiesurin, Dimitris Dimakopoulos, Marco Antonio Sobrevilla Cabezudo, Arash Eshghi, Ioannis Papaioannou, Verena Rieser, and Ioannis Konstas. 2023. The dangers of trusting stochastic parrots: Faithfulness and trust in open-domain conversational question answering. In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, pages 947-959. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 12
                        },
                        {
                            "bbox": [
                                304,
                                443,
                                526,
                                696
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        443,
                                        526,
                                        696
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                443,
                                                526,
                                                696
                                            ],
                                            "type": "text",
                                            "content": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311."
                                        }
                                    ]
                                }
                            ],
                            "index": 13
                        },
                        {
                            "bbox": [
                                304,
                                706,
                                526,
                                739
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        706,
                                        526,
                                        739
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                706,
                                                526,
                                                739
                                            ],
                                            "type": "text",
                                            "content": "Roi Cohen, May Hamri, Mor Geva, and Amir Globerson. 2023. LM vs LM: detecting factual errors via cross examination. CoRR, abs/2305.13281."
                                        }
                                    ]
                                }
                            ],
                            "index": 14
                        },
                        {
                            "bbox": [
                                304,
                                750,
                                526,
                                772
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        750,
                                        526,
                                        772
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                750,
                                                526,
                                                772
                                            ],
                                            "type": "text",
                                            "content": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco"
                                        }
                                    ]
                                }
                            ],
                            "index": 15
                        }
                    ],
                    "sub_type": "ref_text"
                }
            ],
            "discarded_blocks": [],
            "page_size": [
                595,
                841
            ],
            "page_idx": 12
        },
        {
            "para_blocks": [
                {
                    "bbox": [
                        69,
                        72,
                        291,
                        772
                    ],
                    "type": "list",
                    "angle": 0,
                    "index": 9,
                    "blocks": [
                        {
                            "bbox": [
                                80,
                                72,
                                290,
                                150
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        80,
                                        72,
                                        290,
                                        150
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                80,
                                                72,
                                                290,
                                                150
                                            ],
                                            "type": "text",
                                            "content": "Guzmรกn, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 8440-8451. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 0
                        },
                        {
                            "bbox": [
                                69,
                                160,
                                290,
                                194
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        160,
                                        290,
                                        194
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                160,
                                                290,
                                                194
                                            ],
                                            "type": "text",
                                            "content": "Shawn Curran, Sam Lansley, and Oliver Bethell. 2023. Hallucination is the last thing you need. CoRR, abs/2306.11520."
                                        }
                                    ]
                                }
                            ],
                            "index": 1
                        },
                        {
                            "bbox": [
                                69,
                                206,
                                289,
                                250
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        206,
                                        289,
                                        250
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                206,
                                                289,
                                                250
                                            ],
                                            "type": "text",
                                            "content": "Nico Daheim, Nouha Dziri, Mrinmaya Sachan, Iryna Gurevych, and Edoardo M. Ponti. 2023. Elastic weight removal for faithful and abstractive dialogue generation. CoRR, abs/2303.17574."
                                        }
                                    ]
                                }
                            ],
                            "index": 2
                        },
                        {
                            "bbox": [
                                69,
                                262,
                                290,
                                327
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        262,
                                        290,
                                        327
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                262,
                                                290,
                                                327
                                            ],
                                            "type": "text",
                                            "content": "Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven C. H. Hoi. 2023. Instructclip: Towards general-purpose vision-language models with instruction tuning. CoRR, abs/2305.06500."
                                        }
                                    ]
                                }
                            ],
                            "index": 3
                        },
                        {
                            "bbox": [
                                69,
                                339,
                                290,
                                406
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        339,
                                        290,
                                        406
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                339,
                                                290,
                                                406
                                            ],
                                            "type": "text",
                                            "content": "David Dale, Elena Voita, Janice Lam, Prangthip Hansanti, Christophe Ropers, Elahe Kalbassi, Cynthia Gao, Loic Barrault, and Marta R. Costa-jussa. 2023. Halomi: A manually annotated benchmark for multilingual hallucination and omission detection in machine translation. CoRR, abs/2305.11746."
                                        }
                                    ]
                                }
                            ],
                            "index": 4
                        },
                        {
                            "bbox": [
                                69,
                                417,
                                290,
                                495
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        417,
                                        290,
                                        495
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                417,
                                                290,
                                                495
                                            ],
                                            "type": "text",
                                            "content": "Souvik Das, Sougata Saha, and Rohini K. Srihari. 2022. Diving deep into modes of fact hallucinations in dialogue systems. In Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 684-699. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 5
                        },
                        {
                            "bbox": [
                                69,
                                507,
                                290,
                                606
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        507,
                                        290,
                                        606
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                507,
                                                290,
                                                606
                                            ],
                                            "type": "text",
                                            "content": "Debadutta Dash, Rahul Thapa, Juan M. Banda, Akshay Swaminathan, Morgan Cheatham, Mehr Kashyap, Nikesh Kotecha, Jonathan H. Chen, Saurabh Gombar, Lance Downing, Rachel Pedreira, Ethan Goh, Angel Arnaout, Garret Kenn Morris, Honor Magon, Matthew P. Lungren, Eric Horvitz, and Nigam H. Shah. 2023. Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery. CoRR, abs/2304.13714."
                                        }
                                    ]
                                }
                            ],
                            "index": 6
                        },
                        {
                            "bbox": [
                                69,
                                617,
                                290,
                                683
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        617,
                                        290,
                                        683
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                617,
                                                290,
                                                683
                                            ],
                                            "type": "text",
                                            "content": "Zijian Ding, Arvind Srinivasan, Stephen MacNeil, and Joel Chan. 2023. Fluid transformers and creative analogies: Exploring large language models' capacity for augmenting cross-domain analogical creativity. In Creativity and Cognition, C&C 2023, Virtual Event, USA, June 19-21, 2023, pages 489-505. ACM."
                                        }
                                    ]
                                }
                            ],
                            "index": 7
                        },
                        {
                            "bbox": [
                                69,
                                694,
                                291,
                                772
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        694,
                                        291,
                                        772
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                694,
                                                291,
                                                772
                                            ],
                                            "type": "text",
                                            "content": "Tanay Dixit, Fei Wang, and Muhao Chen. 2023. Improving factuality of abstractive summarization without sacrificing summary quality. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 902-913. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 8
                        }
                    ],
                    "sub_type": "ref_text"
                },
                {
                    "bbox": [
                        304,
                        72,
                        525,
                        772
                    ],
                    "type": "list",
                    "angle": 0,
                    "index": 19,
                    "blocks": [
                        {
                            "bbox": [
                                305,
                                72,
                                525,
                                160
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        305,
                                        72,
                                        525,
                                        160
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                305,
                                                72,
                                                525,
                                                160
                                            ],
                                            "type": "text",
                                            "content": "Yue Dong, John Wieting, and Pat Verga. 2022. Faithful to the document or to the world? mitigating hallucinations via entity-linked knowledge in abstractive summarization. In Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 1067-1082. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 10
                        },
                        {
                            "bbox": [
                                304,
                                168,
                                525,
                                212
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        168,
                                        525,
                                        212
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                168,
                                                525,
                                                212
                                            ],
                                            "type": "text",
                                            "content": "Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. 2023. Improving factuality and reasoning in language models through multiagent debate. CoRR, abs/2305.14325."
                                        }
                                    ]
                                }
                            ],
                            "index": 11
                        },
                        {
                            "bbox": [
                                304,
                                220,
                                525,
                                296
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        220,
                                        525,
                                        296
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                220,
                                                525,
                                                296
                                            ],
                                            "type": "text",
                                            "content": "Esin Durmus, He He, and Mona T. Diab. 2020. FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 5055-5070. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 12
                        },
                        {
                            "bbox": [
                                304,
                                305,
                                525,
                                359
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        305,
                                        525,
                                        359
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                305,
                                                525,
                                                359
                                            ],
                                            "type": "text",
                                            "content": "Nouha Dziri, Ehsan Kamalloo, Sivan Milton, Osmar R. Zaรฏane, Mo Yu, Edoardo Maria Ponti, and Siva Reddy. 2022a. Faithdial: A faithful benchmark for information-seeking dialogue. Trans. Assoc. Comput. Linguistics, 10:1473-1490."
                                        }
                                    ]
                                }
                            ],
                            "index": 13
                        },
                        {
                            "bbox": [
                                304,
                                367,
                                525,
                                455
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        367,
                                        525,
                                        455
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                367,
                                                525,
                                                455
                                            ],
                                            "type": "text",
                                            "content": "Nouha Dziri, Andrea Madotto, Osmar Zaiane, and Avishek Joey Bose. 2021. Neural path hunter: Reducing hallucination in dialogue systems via path grounding. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 2197-2214. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 14
                        },
                        {
                            "bbox": [
                                304,
                                463,
                                525,
                                562
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        463,
                                        525,
                                        562
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                463,
                                                525,
                                                562
                                            ],
                                            "type": "text",
                                            "content": "Nouha Dziri, Sivan Milton, Mo Yu, Osmar R. Zaรฏane, and Siva Reddy. 2022b. On the origin of hallucinations in conversational models: Is it the datasets or the models? In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022, pages 5271-5285. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 15
                        },
                        {
                            "bbox": [
                                304,
                                570,
                                525,
                                614
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        570,
                                        525,
                                        614
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                570,
                                                525,
                                                614
                                            ],
                                            "type": "text",
                                            "content": "Nouha Dziri, Hannah Rashkin, Tal Linzen, and David Reitter. 2022c. Evaluating attribution in dialogue systems: The BEGIN benchmark. Trans. Assoc. Comput. Linguistics, 10:1066-1083."
                                        }
                                    ]
                                }
                            ],
                            "index": 16
                        },
                        {
                            "bbox": [
                                304,
                                621,
                                525,
                                654
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        621,
                                        525,
                                        654
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                621,
                                                525,
                                                654
                                            ],
                                            "type": "text",
                                            "content": "Keith Frankish. 2010. Dual-process and dual-system theories of reasoning. Philosophy Compass, 5(10):914-926."
                                        }
                                    ]
                                }
                            ],
                            "index": 17
                        },
                        {
                            "bbox": [
                                304,
                                662,
                                525,
                                772
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        662,
                                        525,
                                        772
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                662,
                                                525,
                                                772
                                            ],
                                            "type": "text",
                                            "content": "Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Y. Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, and Kelvin Guu. 2023a. RARR: researching and revising what language models say, using language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 16477-16508. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 18
                        }
                    ],
                    "sub_type": "ref_text"
                }
            ],
            "discarded_blocks": [],
            "page_size": [
                595,
                841
            ],
            "page_idx": 13
        },
        {
            "para_blocks": [
                {
                    "bbox": [
                        69,
                        72,
                        289,
                        772
                    ],
                    "type": "list",
                    "angle": 0,
                    "index": 10,
                    "blocks": [
                        {
                            "bbox": [
                                69,
                                72,
                                289,
                                149
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        72,
                                        289,
                                        149
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                72,
                                                289,
                                                149
                                            ],
                                            "type": "text",
                                            "content": "Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023b. PAL: program-aided language models. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 10764-10799. PMLR."
                                        }
                                    ]
                                }
                            ],
                            "index": 0
                        },
                        {
                            "bbox": [
                                69,
                                159,
                                289,
                                192
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        159,
                                        289,
                                        192
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                159,
                                                289,
                                                192
                                            ],
                                            "type": "text",
                                            "content": "Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023c. Enabling large language models to generate text with citations. CoRR, abs/2305.14627."
                                        }
                                    ]
                                }
                            ],
                            "index": 1
                        },
                        {
                            "bbox": [
                                69,
                                203,
                                289,
                                269
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        203,
                                        289,
                                        269
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                203,
                                                289,
                                                269
                                            ],
                                            "type": "text",
                                            "content": "Vedant Gaur and Nikunj Saunshi. 2023. Reasoning in large language models through symbolic math word problems. In *Findings of the Association for Computational Linguistics: ACL* 2023, Toronto, Canada, July 9-14, 2023, pages 5889-5903. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 2
                        },
                        {
                            "bbox": [
                                69,
                                280,
                                289,
                                389
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        280,
                                        289,
                                        389
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                280,
                                                289,
                                                389
                                            ],
                                            "type": "text",
                                            "content": "Sukhpal Singh Gill, Minxian Xu, Panos Patros, Huaming Wu, Rupinder Kaur, Kamalpreet Kaur, Stephanie Fuller, Manmeet Singh, Priyansh Arora, Ajith Kumar Parlikad, Vlado Stankovski, Ajith Abraham, Soumya K. Ghosh, Hanan Lutfiyya, Salil S. Kanhere, Rami Bahsoon, Omer F. Rana, Schahram Dustdar, Rizos Sakellariou, Steve Uhlig, and Rajkumar Buyya. 2023. Transformative effects of chatgpt on modern education: Emerging era of AI chatbots. CoRR, abs/2306.03823."
                                        }
                                    ]
                                }
                            ],
                            "index": 3
                        },
                        {
                            "bbox": [
                                69,
                                400,
                                289,
                                444
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        400,
                                        289,
                                        444
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                400,
                                                289,
                                                444
                                            ],
                                            "type": "text",
                                            "content": "Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. 2023. CRITIC: large language models can self-correct with tool-interactive critiquing. CoRR, abs/2305.11738."
                                        }
                                    ]
                                }
                            ],
                            "index": 4
                        },
                        {
                            "bbox": [
                                69,
                                454,
                                289,
                                509
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        454,
                                        289,
                                        509
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                454,
                                                289,
                                                509
                                            ],
                                            "type": "text",
                                            "content": "Nuno Miguel Guerreiro, Duarte M. Alves, Jonas Waldendorf, Barry Haddow, Alexandra Birch, Pierre Colombo, and Andre F. T. Martins. 2023a. Hallucinations in large multilingual translation models. CoRR, abs/2303.16104."
                                        }
                                    ]
                                }
                            ],
                            "index": 5
                        },
                        {
                            "bbox": [
                                69,
                                519,
                                289,
                                608
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        519,
                                        289,
                                        608
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                519,
                                                289,
                                                608
                                            ],
                                            "type": "text",
                                            "content": "Nuno Miguel Guerreiro, Elena Voita, and Andre F. T. Martins. 2023b. Looking for a needle in a haystack: A comprehensive study of hallucinations in neural machine translation. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2023, Dubrovnik, Croatia, May 2-6, 2023, pages 1059-1075. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 6
                        },
                        {
                            "bbox": [
                                69,
                                618,
                                289,
                                662
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        618,
                                        289,
                                        662
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                618,
                                                289,
                                                662
                                            ],
                                            "type": "text",
                                            "content": "Honghao Gui, Jintian Zhang, Hongbin Ye, and Ningyu Zhang. 2023. Instructie: A chinese instruction-based information extraction dataset. CoRR, abs/2305.11527."
                                        }
                                    ]
                                }
                            ],
                            "index": 7
                        },
                        {
                            "bbox": [
                                69,
                                673,
                                289,
                                717
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        673,
                                        289,
                                        717
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                673,
                                                289,
                                                717
                                            ],
                                            "type": "text",
                                            "content": "Danny Halawi, Jean-Stanislas Denain, and Jacob Steinhardt. 2023. Overthinking the truth: Understanding how language models process false demonstrations. CoRR, abs/2307.09476."
                                        }
                                    ]
                                }
                            ],
                            "index": 8
                        },
                        {
                            "bbox": [
                                69,
                                728,
                                289,
                                772
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        728,
                                        289,
                                        772
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                728,
                                                289,
                                                772
                                            ],
                                            "type": "text",
                                            "content": "Xiaochuang Han and Yulia Tsvetkov. 2022. ORCA: interpreting prompted language models via locating supporting data evidence in the ocean of pretraining data. CoRR, abs/2205.12600."
                                        }
                                    ]
                                }
                            ],
                            "index": 9
                        }
                    ],
                    "sub_type": "ref_text"
                },
                {
                    "bbox": [
                        304,
                        72,
                        524,
                        772
                    ],
                    "type": "list",
                    "angle": 0,
                    "index": 21,
                    "blocks": [
                        {
                            "bbox": [
                                304,
                                72,
                                524,
                                159
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        72,
                                        524,
                                        159
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                72,
                                                524,
                                                159
                                            ],
                                            "type": "text",
                                            "content": "Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi Huo, Jiezhong Qiu, Yuan Yao, Ao Zhang, Liang Zhang, Wentao Han, Minlie Huang, Qin Jin, Yanyan Lan, Yang Liu, Zhiyuan Liu, Zhiwu Lu, Xipeng Qiu, Ruihua Song, Jie Tang, Ji-Rong Wen, Jinhui Yuan, Wayne Xin Zhao, and Jun Zhu. 2021. Pre-trained models: Past, present and future. AI Open, 2:225-250."
                                        }
                                    ]
                                }
                            ],
                            "index": 11
                        },
                        {
                            "bbox": [
                                304,
                                168,
                                524,
                                268
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        168,
                                        524,
                                        268
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                168,
                                                524,
                                                268
                                            ],
                                            "type": "text",
                                            "content": "Peter Hase, Mona T. Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and Srinivasan Iyer. 2023. Methods for measuring, updating, and visualizing factual beliefs in language models. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2023, Dubrovnik, Croatia, May 2-6, 2023, pages 2706-2723. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 12
                        },
                        {
                            "bbox": [
                                304,
                                275,
                                524,
                                307
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        275,
                                        524,
                                        307
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                275,
                                                524,
                                                307
                                            ],
                                            "type": "text",
                                            "content": "Hangfeng He, Hongming Zhang, and Dan Roth. 2023. Rethinking with retrieval: Faithful large language model inference. CoRR, abs/2301.00303."
                                        }
                                    ]
                                }
                            ],
                            "index": 13
                        },
                        {
                            "bbox": [
                                304,
                                316,
                                524,
                                381
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        316,
                                        524,
                                        381
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                316,
                                                524,
                                                381
                                            ],
                                            "type": "text",
                                            "content": "Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Affify, and Hany Hassan Awadalla. 2023. How good are GPT models at machine translation? A comprehensive evaluation. CoRR, abs/2302.09210."
                                        }
                                    ]
                                }
                            ],
                            "index": 14
                        },
                        {
                            "bbox": [
                                304,
                                390,
                                524,
                                456
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        390,
                                        524,
                                        456
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                390,
                                                524,
                                                456
                                            ],
                                            "type": "text",
                                            "content": "Jie Huang and Kevin Chen-Chuan Chang. 2023. Towards reasoning in large language models: A survey. In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, pages 1049-1065. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 15
                        },
                        {
                            "bbox": [
                                304,
                                464,
                                524,
                                542
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        464,
                                        524,
                                        542
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                464,
                                                524,
                                                542
                                            ],
                                            "type": "text",
                                            "content": "Kung-Hsiang Huang, Hou Pong Chan, and Heng Ji. 2023. Zero-shot faithful factual error correction. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 5660-5676. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 16
                        },
                        {
                            "bbox": [
                                304,
                                550,
                                524,
                                582
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        550,
                                        524,
                                        582
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                550,
                                                524,
                                                582
                                            ],
                                            "type": "text",
                                            "content": "Siqing Huo, Negar Arabzadeh, and Charles L. A. Clarke. 2023. Retrieving supporting evidence for llms generated answers. CoRR, abs/2306.13781."
                                        }
                                    ]
                                }
                            ],
                            "index": 17
                        },
                        {
                            "bbox": [
                                304,
                                590,
                                524,
                                656
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        590,
                                        524,
                                        656
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                590,
                                                524,
                                                656
                                            ],
                                            "type": "text",
                                            "content": "Gabriel Ilharco, Marco Tรบlio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. 2023. Editing models with task arithmetic. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net."
                                        }
                                    ]
                                }
                            ],
                            "index": 18
                        },
                        {
                            "bbox": [
                                304,
                                664,
                                524,
                                719
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        664,
                                        524,
                                        719
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                664,
                                                524,
                                                719
                                            ],
                                            "type": "text",
                                            "content": "Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM Comput. Surv., 55(12):248:1-248:38."
                                        }
                                    ]
                                }
                            ],
                            "index": 19
                        },
                        {
                            "bbox": [
                                304,
                                728,
                                524,
                                772
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        728,
                                        524,
                                        772
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                728,
                                                524,
                                                772
                                            ],
                                            "type": "text",
                                            "content": "Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active retrieval augmented generation. CoRR, abs/2305.06983."
                                        }
                                    ]
                                }
                            ],
                            "index": 20
                        }
                    ],
                    "sub_type": "ref_text"
                }
            ],
            "discarded_blocks": [],
            "page_size": [
                595,
                841
            ],
            "page_idx": 14
        },
        {
            "para_blocks": [
                {
                    "bbox": [
                        69,
                        72,
                        289,
                        771
                    ],
                    "type": "list",
                    "angle": 0,
                    "index": 9,
                    "blocks": [
                        {
                            "bbox": [
                                69,
                                72,
                                289,
                                116
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        72,
                                        289,
                                        116
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                72,
                                                289,
                                                116
                                            ],
                                            "type": "text",
                                            "content": "Qiao Jin, Yifan Yang, Qingyu Chen, and Zhiyong Lu. 2023. Genegpt: Augmenting large language models with domain tools for improved access to biomedical information. CoRR, abs/2304.09667."
                                        }
                                    ]
                                }
                            ],
                            "index": 0
                        },
                        {
                            "bbox": [
                                69,
                                125,
                                289,
                                268
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        125,
                                        289,
                                        268
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                125,
                                                289,
                                                268
                                            ],
                                            "type": "text",
                                            "content": "Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. 2022. Language models (mostly) know what they know. CoRR, abs/2207.05221."
                                        }
                                    ]
                                }
                            ],
                            "index": 1
                        },
                        {
                            "bbox": [
                                69,
                                277,
                                289,
                                354
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        277,
                                        289,
                                        354
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                277,
                                                289,
                                                354
                                            ],
                                            "type": "text",
                                            "content": "Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2023. Large language models struggle to learn long-tail knowledge. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 15696-15707. PMLR."
                                        }
                                    ]
                                }
                            ],
                            "index": 2
                        },
                        {
                            "bbox": [
                                69,
                                363,
                                289,
                                429
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        363,
                                        289,
                                        429
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                363,
                                                289,
                                                429
                                            ],
                                            "type": "text",
                                            "content": "Daniel Kang and Tatsunori Hashimoto. 2020. Improved natural language generation via loss truncation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 718-731. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 3
                        },
                        {
                            "bbox": [
                                69,
                                439,
                                289,
                                515
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        439,
                                        289,
                                        515
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                439,
                                                289,
                                                515
                                            ],
                                            "type": "text",
                                            "content": "Yuval Kirstain, Patrick S. H. Lewis, Sebastian Riedel, and Omer Levy. 2022. A few more examples may be worth billions of parameters. In Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 1017-1029. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 4
                        },
                        {
                            "bbox": [
                                69,
                                524,
                                289,
                                558
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        524,
                                        289,
                                        558
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                524,
                                                289,
                                                558
                                            ],
                                            "type": "text",
                                            "content": "Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In NeurIPS."
                                        }
                                    ]
                                }
                            ],
                            "index": 5
                        },
                        {
                            "bbox": [
                                69,
                                566,
                                289,
                                621
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        566,
                                        289,
                                        621
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                566,
                                                289,
                                                621
                                            ],
                                            "type": "text",
                                            "content": "Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. 2022. Internet-augmented language models through few-shot prompting for open-domain question answering. CoRR, abs/2203.05115."
                                        }
                                    ]
                                }
                            ],
                            "index": 6
                        },
                        {
                            "bbox": [
                                69,
                                630,
                                289,
                                675
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        630,
                                        289,
                                        675
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                630,
                                                289,
                                                675
                                            ],
                                            "type": "text",
                                            "content": "Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pascale Fung, Mohammad Shoeybi, and Bryan Catanzaro. 2022. Factuality enhanced language models for open-ended text generation. In NeurIPS."
                                        }
                                    ]
                                }
                            ],
                            "index": 7
                        },
                        {
                            "bbox": [
                                69,
                                684,
                                289,
                                771
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        684,
                                        289,
                                        771
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                684,
                                                289,
                                                771
                                            ],
                                            "type": "text",
                                            "content": "Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. 2023a. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 19730-19742. PMLR."
                                        }
                                    ]
                                }
                            ],
                            "index": 8
                        }
                    ],
                    "sub_type": "ref_text"
                },
                {
                    "bbox": [
                        304,
                        72,
                        524,
                        772
                    ],
                    "type": "list",
                    "angle": 0,
                    "index": 22,
                    "blocks": [
                        {
                            "bbox": [
                                304,
                                72,
                                524,
                                116
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        72,
                                        524,
                                        116
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                72,
                                                524,
                                                116
                                            ],
                                            "type": "text",
                                            "content": "Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023b. Halueval: A large-scale hallucination evaluation benchmark for large language models. CoRR, abs/2305.11747."
                                        }
                                    ]
                                }
                            ],
                            "index": 10
                        },
                        {
                            "bbox": [
                                304,
                                126,
                                524,
                                170
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        126,
                                        524,
                                        170
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                126,
                                                524,
                                                170
                                            ],
                                            "type": "text",
                                            "content": "Kenneth Li, Oam Patel, Fernanda B. Viรฉgas, Hanspeter Pfister, and Martin Wattenberg. 2023c. Inference-time intervention: Eliciting truthful answers from a language model. CoRR, abs/2306.03341."
                                        }
                                    ]
                                }
                            ],
                            "index": 11
                        },
                        {
                            "bbox": [
                                304,
                                179,
                                524,
                                213
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        179,
                                        524,
                                        213
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                179,
                                                524,
                                                213
                                            ],
                                            "type": "text",
                                            "content": "Miaoran Li, Baolin Peng, and Zhu Zhang. 2023d. Self-checker: Plug-and-play modules for fact-checking with large language models. CoRR, abs/2305.14623."
                                        }
                                    ]
                                }
                            ],
                            "index": 12
                        },
                        {
                            "bbox": [
                                304,
                                222,
                                524,
                                255
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        222,
                                        524,
                                        255
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                222,
                                                524,
                                                255
                                            ],
                                            "type": "text",
                                            "content": "Ruosen Li, Teerth Patel, and Xinya Du. 2023e. PRD: peer rank and discussion improve large language model based evaluations. CoRR, abs/2307.02762."
                                        }
                                    ]
                                }
                            ],
                            "index": 13
                        },
                        {
                            "bbox": [
                                304,
                                264,
                                524,
                                297
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        264,
                                        524,
                                        297
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                264,
                                                524,
                                                297
                                            ],
                                            "type": "text",
                                            "content": "Shuo Li, Sangdon Park, Insup Lee, and Osbert Bastani. 2023f. TRAC: trustworthy retrieval augmented chatbot. CoRR, abs/2307.04642."
                                        }
                                    ]
                                }
                            ],
                            "index": 14
                        },
                        {
                            "bbox": [
                                304,
                                307,
                                524,
                                351
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        307,
                                        524,
                                        351
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                307,
                                                524,
                                                351
                                            ],
                                            "type": "text",
                                            "content": "Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, and Mike Lewis. 2023g. Self-alignment with instruction back-translation. CoRR, abs/2308.06259."
                                        }
                                    ]
                                }
                            ],
                            "index": 15
                        },
                        {
                            "bbox": [
                                304,
                                361,
                                524,
                                416
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        361,
                                        524,
                                        416
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                361,
                                                524,
                                                416
                                            ],
                                            "type": "text",
                                            "content": "Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Lidong Bing, Shafiq R. Joty, and Soujanya Poria. 2023h. Chain of knowledge: A framework for grounding large language models with structured knowledge bases. CoRR, abs/2305.13269."
                                        }
                                    ]
                                }
                            ],
                            "index": 16
                        },
                        {
                            "bbox": [
                                304,
                                426,
                                524,
                                470
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        426,
                                        524,
                                        470
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                426,
                                                524,
                                                470
                                            ],
                                            "type": "text",
                                            "content": "Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. 2023i. Evaluating object hallucination in large vision-language models. CoRR, abs/2305.10355."
                                        }
                                    ]
                                }
                            ],
                            "index": 17
                        },
                        {
                            "bbox": [
                                304,
                                480,
                                524,
                                578
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        480,
                                        524,
                                        578
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                480,
                                                524,
                                                578
                                            ],
                                            "type": "text",
                                            "content": "Yu Li, Baolin Peng, Yelong Shen, Yi Mao, Lars Liden, Zhou Yu, and Jianfeng Gao. 2022. Knowledge-grounded dialogue generation with a unified knowledge representation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022, pages 206-218. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 18
                        },
                        {
                            "bbox": [
                                304,
                                587,
                                524,
                                642
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        587,
                                        524,
                                        642
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                587,
                                                524,
                                                642
                                            ],
                                            "type": "text",
                                            "content": "Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. 2023. Encouraging divergent thinking in large language models through multi-agent debate. CoRR, abs/2305.19118."
                                        }
                                    ]
                                }
                            ],
                            "index": 19
                        },
                        {
                            "bbox": [
                                304,
                                652,
                                524,
                                729
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        652,
                                        524,
                                        729
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                652,
                                                524,
                                                729
                                            ],
                                            "type": "text",
                                            "content": "Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. Truthfulqa: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 3214-3252. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 20
                        },
                        {
                            "bbox": [
                                304,
                                739,
                                524,
                                772
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        739,
                                        524,
                                        772
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                739,
                                                524,
                                                772
                                            ],
                                            "type": "text",
                                            "content": "Hao Liu, Carmelo Sferrazza, and Pieter Abbeel. 2023a. Chain of hindsight aligns language models with feedback. CoRR, abs/2302.02676."
                                        }
                                    ]
                                }
                            ],
                            "index": 21
                        }
                    ],
                    "sub_type": "ref_text"
                }
            ],
            "discarded_blocks": [],
            "page_size": [
                595,
                841
            ],
            "page_idx": 15
        },
        {
            "para_blocks": [
                {
                    "bbox": [
                        69,
                        72,
                        289,
                        772
                    ],
                    "type": "list",
                    "angle": 0,
                    "index": 11,
                    "blocks": [
                        {
                            "bbox": [
                                69,
                                72,
                                289,
                                105
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        72,
                                        289,
                                        105
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                72,
                                                289,
                                                105
                                            ],
                                            "type": "text",
                                            "content": "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. Visual instruction tuning. CoRR, abs/2304.08485."
                                        }
                                    ]
                                }
                            ],
                            "index": 0
                        },
                        {
                            "bbox": [
                                69,
                                114,
                                289,
                                158
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        114,
                                        289,
                                        158
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                114,
                                                289,
                                                158
                                            ],
                                            "type": "text",
                                            "content": "Jiongnan Liu, Jiajie Jin, Zihan Wang, Jiehan Cheng, Zhicheng Dou, and Ji-Rong Wen. 2023c. RETALLM: A retrieval-augmented large language model toolkit. CoRR, abs/2306.05212."
                                        }
                                    ]
                                }
                            ],
                            "index": 1
                        },
                        {
                            "bbox": [
                                69,
                                166,
                                289,
                                243
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        166,
                                        289,
                                        243
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                166,
                                                289,
                                                243
                                            ],
                                            "type": "text",
                                            "content": "Yixin Liu, Pengfei Liu, Dragomir R. Radev, and Graham Neubig. 2022. BRIO: bringing order to abstractive summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 2890-2903. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 2
                        },
                        {
                            "bbox": [
                                69,
                                251,
                                289,
                                339
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        251,
                                        289,
                                        339
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                251,
                                                289,
                                                339
                                            ],
                                            "type": "text",
                                            "content": "Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 8086-8098. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 3
                        },
                        {
                            "bbox": [
                                69,
                                348,
                                289,
                                391
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        348,
                                        289,
                                        391
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                348,
                                                289,
                                                391
                                            ],
                                            "type": "text",
                                            "content": "Ziyang Luo, Can Xu, Pu Zhao, Xiubo Geng, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. Augmented large language models with parametric knowledge guiding. CoRR, abs/2305.04757."
                                        }
                                    ]
                                }
                            ],
                            "index": 4
                        },
                        {
                            "bbox": [
                                69,
                                400,
                                289,
                                476
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        400,
                                        289,
                                        476
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                400,
                                                289,
                                                476
                                            ],
                                            "type": "text",
                                            "content": "Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. CoRR, abs/2303.17651."
                                        }
                                    ]
                                }
                            ],
                            "index": 5
                        },
                        {
                            "bbox": [
                                69,
                                485,
                                289,
                                518
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        485,
                                        289,
                                        518
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                485,
                                                289,
                                                518
                                            ],
                                            "type": "text",
                                            "content": "Razi Mahmood, Ge Wang, Mannudeep K. Kalra, and Pingkun Yan. 2023. Fact-checking of ai-generated reports. CoRR, abs/2307.14634."
                                        }
                                    ]
                                }
                            ],
                            "index": 6
                        },
                        {
                            "bbox": [
                                69,
                                527,
                                289,
                                625
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        527,
                                        289,
                                        625
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                527,
                                                289,
                                                625
                                            ],
                                            "type": "text",
                                            "content": "Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 9802-9822. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 7
                        },
                        {
                            "bbox": [
                                69,
                                634,
                                289,
                                677
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        634,
                                        289,
                                        677
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                634,
                                                289,
                                                677
                                            ],
                                            "type": "text",
                                            "content": "Potsawee Manakul, Adian Liusie, and Mark J. F. Gales. 2023. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. CoRR, abs/2303.08896."
                                        }
                                    ]
                                }
                            ],
                            "index": 8
                        },
                        {
                            "bbox": [
                                69,
                                686,
                                289,
                                719
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        686,
                                        289,
                                        719
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                686,
                                                289,
                                                719
                                            ],
                                            "type": "text",
                                            "content": "Marc Marone and Benjamin Van Durme. 2023. Data portraits: Recording foundation model training data. CoRR, abs/2303.03919."
                                        }
                                    ]
                                }
                            ],
                            "index": 9
                        },
                        {
                            "bbox": [
                                69,
                                728,
                                289,
                                772
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        728,
                                        289,
                                        772
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                728,
                                                289,
                                                772
                                            ],
                                            "type": "text",
                                            "content": "Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan T. McDonald. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for"
                                        }
                                    ]
                                }
                            ],
                            "index": 10
                        }
                    ],
                    "sub_type": "ref_text"
                },
                {
                    "bbox": [
                        304,
                        72,
                        524,
                        772
                    ],
                    "type": "list",
                    "angle": 0,
                    "index": 23,
                    "blocks": [
                        {
                            "bbox": [
                                314,
                                72,
                                524,
                                105
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        314,
                                        72,
                                        524,
                                        105
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                314,
                                                72,
                                                524,
                                                105
                                            ],
                                            "type": "text",
                                            "content": "Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 1906-1919. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 12
                        },
                        {
                            "bbox": [
                                304,
                                114,
                                524,
                                168
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        114,
                                        524,
                                        168
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                114,
                                                524,
                                                168
                                            ],
                                            "type": "text",
                                            "content": "Nick McKenna, Tianyi Li, Liang Cheng, Mohammad Javad Hosseini, Mark Johnson, and Mark Steedman. 2023. Sources of hallucination by large language models on inference tasks. CoRR, abs/2305.14552."
                                        }
                                    ]
                                }
                            ],
                            "index": 13
                        },
                        {
                            "bbox": [
                                304,
                                177,
                                524,
                                243
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        177,
                                        524,
                                        243
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                177,
                                                524,
                                                243
                                            ],
                                            "type": "text",
                                            "content": "Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, H. Francis Song, Martin J. Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, and Nat McAleese. 2022. Teaching language models to support answers with verified quotes. CoRR, abs/2203.11147."
                                        }
                                    ]
                                }
                            ],
                            "index": 14
                        },
                        {
                            "bbox": [
                                304,
                                252,
                                524,
                                295
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        252,
                                        524,
                                        295
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                252,
                                                524,
                                                295
                                            ],
                                            "type": "text",
                                            "content": "Nandana Mihindukulasooriya, Sanju Tiwari, Carlos F. Enguix, and Kusum Lata. 2023. Text2kgbench: A benchmark for ontology-driven knowledge graph generation from text. CoRR, abs/2308.02357."
                                        }
                                    ]
                                }
                            ],
                            "index": 15
                        },
                        {
                            "bbox": [
                                304,
                                304,
                                524,
                                369
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        304,
                                        524,
                                        369
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                304,
                                                524,
                                                369
                                            ],
                                            "type": "text",
                                            "content": "Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. CoRR, abs/2305.14251."
                                        }
                                    ]
                                }
                            ],
                            "index": 16
                        },
                        {
                            "bbox": [
                                304,
                                378,
                                524,
                                400
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        378,
                                        524,
                                        400
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                378,
                                                524,
                                                400
                                            ],
                                            "type": "text",
                                            "content": "Marvin Minsky. 1988. Society of mind. Simon and Schuster."
                                        }
                                    ]
                                }
                            ],
                            "index": 17
                        },
                        {
                            "bbox": [
                                304,
                                408,
                                524,
                                453
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        408,
                                        524,
                                        453
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                408,
                                                524,
                                                453
                                            ],
                                            "type": "text",
                                            "content": "Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2021. Natural instructions: Benchmarking generalization to new tasks from natural language instructions. CoRR, abs/2104.08773."
                                        }
                                    ]
                                }
                            ],
                            "index": 18
                        },
                        {
                            "bbox": [
                                304,
                                461,
                                524,
                                505
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        461,
                                        524,
                                        505
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                461,
                                                524,
                                                505
                                            ],
                                            "type": "text",
                                            "content": "Niels Mรผndler, Jingxuan He, Slobodan Jenko, and Martin T. Vechev. 2023. Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation. CoRR, abs/2305.15852."
                                        }
                                    ]
                                }
                            ],
                            "index": 19
                        },
                        {
                            "bbox": [
                                304,
                                513,
                                524,
                                567
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        513,
                                        524,
                                        567
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                513,
                                                524,
                                                567
                                            ],
                                            "type": "text",
                                            "content": "Munan Ning, Yujia Xie, Dongdong Chen, Zeyin Song, Lu Yuan, Yonghong Tian, Qixiang Ye, and Li Yuan. 2023. Album storytelling with iterative story-aware captioning and large language models. CoRR, abs/2305.12943."
                                        }
                                    ]
                                }
                            ],
                            "index": 20
                        },
                        {
                            "bbox": [
                                304,
                                576,
                                524,
                                664
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        576,
                                        524,
                                        664
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                576,
                                                524,
                                                664
                                            ],
                                            "type": "text",
                                            "content": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In NeurIPS."
                                        }
                                    ]
                                }
                            ],
                            "index": 21
                        },
                        {
                            "bbox": [
                                304,
                                673,
                                524,
                                772
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        673,
                                        524,
                                        772
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                673,
                                                524,
                                                772
                                            ],
                                            "type": "text",
                                            "content": "Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pages 4812-4829. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 22
                        }
                    ],
                    "sub_type": "ref_text"
                }
            ],
            "discarded_blocks": [],
            "page_size": [
                595,
                841
            ],
            "page_idx": 16
        },
        {
            "para_blocks": [
                {
                    "bbox": [
                        69,
                        72,
                        291,
                        772
                    ],
                    "type": "list",
                    "angle": 0,
                    "index": 10,
                    "blocks": [
                        {
                            "bbox": [
                                69,
                                72,
                                291,
                                127
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        72,
                                        291,
                                        127
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                72,
                                                291,
                                                127
                                            ],
                                            "type": "text",
                                            "content": "Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. 2023. Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. CoRR, abs/2308.03188."
                                        }
                                    ]
                                }
                            ],
                            "index": 0
                        },
                        {
                            "bbox": [
                                69,
                                139,
                                290,
                                216
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        139,
                                        290,
                                        216
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                139,
                                                290,
                                                216
                                            ],
                                            "type": "text",
                                            "content": "Sung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Madry. 2023. TRAK: attributing model behavior at scale. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 27074-27113. PMLR."
                                        }
                                    ]
                                }
                            ],
                            "index": 1
                        },
                        {
                            "bbox": [
                                69,
                                227,
                                290,
                                271
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        227,
                                        290,
                                        271
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                227,
                                                290,
                                                271
                                            ],
                                            "type": "text",
                                            "content": "Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. 2023. Gorilla: Large language model connected with massive apis. CoRR, abs/2305.15334."
                                        }
                                    ]
                                }
                            ],
                            "index": 2
                        },
                        {
                            "bbox": [
                                69,
                                283,
                                290,
                                372
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        283,
                                        290,
                                        372
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                283,
                                                290,
                                                372
                                            ],
                                            "type": "text",
                                            "content": "Jiaxin Pei and David Jurgens. 2021. Measuring sentence-level and aspect-level (un)certainty in science communications. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 9959-10011. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 3
                        },
                        {
                            "bbox": [
                                69,
                                383,
                                290,
                                449
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        383,
                                        290,
                                        449
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                383,
                                                290,
                                                449
                                            ],
                                            "type": "text",
                                            "content": "Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, and Jianfeng Gao. 2023. Check your facts and try again: Improving large language models with external knowledge and automated feedback. CoRR, abs/2302.12813."
                                        }
                                    ]
                                }
                            ],
                            "index": 4
                        },
                        {
                            "bbox": [
                                69,
                                460,
                                290,
                                505
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        460,
                                        290,
                                        505
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                460,
                                                290,
                                                505
                                            ],
                                            "type": "text",
                                            "content": "Suzanne Petryk, Spencer Whitehead, Joseph E. Gonzalez, Trevor Darrell, Anna Rohrbach, and Marcus Rohrbach. 2023. Simple token-level confidence improves caption correctness. CoRR, abs/2305.07021."
                                        }
                                    ]
                                }
                            ],
                            "index": 5
                        },
                        {
                            "bbox": [
                                69,
                                516,
                                290,
                                550
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        516,
                                        290,
                                        550
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                516,
                                                290,
                                                550
                                            ],
                                            "type": "text",
                                            "content": "Pouya Pezeshkpour. 2023. Measuring and modifying factual knowledge in large language models. CoRR, abs/2306.06264."
                                        }
                                    ]
                                }
                            ],
                            "index": 6
                        },
                        {
                            "bbox": [
                                69,
                                561,
                                290,
                                616
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        561,
                                        290,
                                        616
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                561,
                                                290,
                                                616
                                            ],
                                            "type": "text",
                                            "content": "Jonas Pfeiffer, Francesco Piccinno, Massimo Nicosia, Xinyi Wang, Machel Reid, and Sebastian Ruder. 2023. mmt5: Modular multilingual pre-training solves source language hallucinations. CoRR, abs/2305.14224."
                                        }
                                    ]
                                }
                            ],
                            "index": 7
                        },
                        {
                            "bbox": [
                                69,
                                628,
                                290,
                                672
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        628,
                                        290,
                                        672
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                628,
                                                290,
                                                672
                                            ],
                                            "type": "text",
                                            "content": "Gualtiero Piccinini. 2004. The first computational theory of mind and brain: a close look at mcculloch and pitts's \"logical calculus of ideas immanent in nervous activity\". Synthese, 141:175-215."
                                        }
                                    ]
                                }
                            ],
                            "index": 8
                        },
                        {
                            "bbox": [
                                69,
                                684,
                                290,
                                772
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        684,
                                        290,
                                        772
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                684,
                                                290,
                                                772
                                            ],
                                            "type": "text",
                                            "content": "Dongqi Pu and Vera Demberg. 2023. Chatgpt vs human-authored text: Insights into controllable text summarization and sentence style transfer. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, ACL 2023, Toronto, Canada, July 9-14, 2023, pages 1-18. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 9
                        }
                    ],
                    "sub_type": "ref_text"
                },
                {
                    "bbox": [
                        304,
                        72,
                        525,
                        772
                    ],
                    "type": "list",
                    "angle": 0,
                    "index": 21,
                    "blocks": [
                        {
                            "bbox": [
                                305,
                                72,
                                525,
                                116
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        305,
                                        72,
                                        525,
                                        116
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                305,
                                                72,
                                                525,
                                                116
                                            ],
                                            "type": "text",
                                            "content": "Yifu Qiu, Yftah Ziser, Anna Korhonen, Edoardo M. Ponti, and Shay B. Cohen. 2023. Detecting and mitigating hallucinations in multilingual summarisation. CoRR, abs/2305.13632."
                                        }
                                    ]
                                }
                            ],
                            "index": 11
                        },
                        {
                            "bbox": [
                                304,
                                125,
                                525,
                                215
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        125,
                                        525,
                                        215
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                125,
                                                525,
                                                215
                                            ],
                                            "type": "text",
                                            "content": "Vikas Raunak, Arul Menezes, and Marcin Junczys-Dowmunt. 2021. The curious case of hallucinations in neural machine translation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pages 1172-1183. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 12
                        },
                        {
                            "bbox": [
                                304,
                                222,
                                525,
                                409
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        222,
                                        525,
                                        409
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                222,
                                                525,
                                                409
                                            ],
                                            "type": "text",
                                            "content": "Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. 2022. Multi-task prompted training enables zero-shot task generalization. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net."
                                        }
                                    ]
                                }
                            ],
                            "index": 13
                        },
                        {
                            "bbox": [
                                304,
                                417,
                                525,
                                473
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        417,
                                        525,
                                        473
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                417,
                                                525,
                                                473
                                            ],
                                            "type": "text",
                                            "content": "Wenqi Shao, Yutao Hu, Peng Gao, Meng Lei, Kaipeng Zhang, Fanqing Meng, Peng Xu, Siyuan Huang, Hongsheng Li, Yu Qiao, and Ping Luo. 2023. Tiny lvlm-ehub: Early multimodal experiments with bard. CoRR, abs/2308.03729."
                                        }
                                    ]
                                }
                            ],
                            "index": 14
                        },
                        {
                            "bbox": [
                                304,
                                482,
                                525,
                                559
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        482,
                                        525,
                                        559
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                482,
                                                525,
                                                559
                                            ],
                                            "type": "text",
                                            "content": "Jiaming Shen, Jialu Liu, Daniel Finnie, Negar Rahmati, Mike Bendersky, and Marc Najork. 2023. \"why is this misleading?\": Detecting news headline hallucinations with explanations. In Proceedings of the ACM Web Conference 2023, WWW 2023, Austin, TX, USA, 30 April 2023 - 4 May 2023, pages 1662-1672. ACM."
                                        }
                                    ]
                                }
                            ],
                            "index": 15
                        },
                        {
                            "bbox": [
                                304,
                                568,
                                525,
                                613
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        568,
                                        525,
                                        613
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                568,
                                                525,
                                                613
                                            ],
                                            "type": "text",
                                            "content": "Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer, and Scott Wen-tau Yih. 2023a. Trusting your evidence: Hallucinate less with context-aware decoding. CoRR, abs/2305.14739."
                                        }
                                    ]
                                }
                            ],
                            "index": 16
                        },
                        {
                            "bbox": [
                                304,
                                622,
                                525,
                                666
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        622,
                                        525,
                                        666
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                622,
                                                525,
                                                666
                                            ],
                                            "type": "text",
                                            "content": "Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023b. REPLUG: retrieval-augmented black-box language models. CoRR, abs/2301.12652."
                                        }
                                    ]
                                }
                            ],
                            "index": 17
                        },
                        {
                            "bbox": [
                                304,
                                676,
                                525,
                                709
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        676,
                                        525,
                                        709
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                676,
                                                525,
                                                709
                                            ],
                                            "type": "text",
                                            "content": "Noah Shinn, Beck Labash, and Ashwin Gopinath. 2023. Reflexion: an autonomous agent with dynamic memory and self-reflection. CoRR, abs/2303.11366."
                                        }
                                    ]
                                }
                            ],
                            "index": 18
                        },
                        {
                            "bbox": [
                                304,
                                718,
                                525,
                                740
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        718,
                                        525,
                                        740
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                718,
                                                525,
                                                740
                                            ],
                                            "type": "text",
                                            "content": "Keith Stanovich. 2011. Rationality and the reflective mind. Oxford University Press, USA."
                                        }
                                    ]
                                }
                            ],
                            "index": 19
                        },
                        {
                            "bbox": [
                                304,
                                749,
                                525,
                                772
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        749,
                                        525,
                                        772
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                749,
                                                525,
                                                772
                                            ],
                                            "type": "text",
                                            "content": "Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,"
                                        }
                                    ]
                                }
                            ],
                            "index": 20
                        }
                    ],
                    "sub_type": "ref_text"
                }
            ],
            "discarded_blocks": [],
            "page_size": [
                595,
                841
            ],
            "page_idx": 17
        },
        {
            "para_blocks": [
                {
                    "bbox": [
                        69,
                        72,
                        291,
                        772
                    ],
                    "type": "list",
                    "angle": 0,
                    "index": 10,
                    "blocks": [
                        {
                            "bbox": [
                                80,
                                72,
                                291,
                                138
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        80,
                                        72,
                                        291,
                                        138
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                80,
                                                72,
                                                291,
                                                138
                                            ],
                                            "type": "text",
                                            "content": "Dario Amodei, and Paul F. Christiano. 2020. Learning to summarize with human feedback. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual."
                                        }
                                    ]
                                }
                            ],
                            "index": 0
                        },
                        {
                            "bbox": [
                                69,
                                146,
                                291,
                                255
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        146,
                                        291,
                                        255
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                146,
                                                291,
                                                255
                                            ],
                                            "type": "text",
                                            "content": "Weiwei Sun, Zhengliang Shi, Shen Gao, Pengjie Ren, Maarten de Rijke, and Zhaochun Ren. 2023. Contrastive learning reduces hallucination in conversations. In Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2023, Washington, DC, USA, February 7-14, 2023, pages 13618-13626. AAAI Press."
                                        }
                                    ]
                                }
                            ],
                            "index": 1
                        },
                        {
                            "bbox": [
                                69,
                                264,
                                291,
                                342
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        264,
                                        291,
                                        342
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                264,
                                                291,
                                                342
                                            ],
                                            "type": "text",
                                            "content": "Derek Tam, Anisha Mascarenhas, Shiyue Zhang, Sarah Kwan, Mohit Bansal, and Colin Raffel. 2023. Evaluating the factual consistency of large language models through news summarization. In *Findings of the Association for Computational Linguistics: ACL* 2023, Toronto, Canada, July 9-14, 2023, pages 5220-5255. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 2
                        },
                        {
                            "bbox": [
                                69,
                                349,
                                290,
                                371
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        349,
                                        290,
                                        371
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                349,
                                                290,
                                                371
                                            ],
                                            "type": "text",
                                            "content": "Edward L Thorndike. 1898. Animal intelligence. Nature, 58(1504):390-390."
                                        }
                                    ]
                                }
                            ],
                            "index": 3
                        },
                        {
                            "bbox": [
                                69,
                                380,
                                290,
                                455
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        380,
                                        290,
                                        455
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                380,
                                                290,
                                                455
                                            ],
                                            "type": "text",
                                            "content": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothรฉe Lacroix, Baptiste Roziรจre, Naman Goyal, Eric Hambro, Faisal Azhar, Aurรฉlien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971."
                                        }
                                    ]
                                }
                            ],
                            "index": 4
                        },
                        {
                            "bbox": [
                                69,
                                464,
                                291,
                                564
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        464,
                                        291,
                                        564
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                464,
                                                291,
                                                564
                                            ],
                                            "type": "text",
                                            "content": "Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2023. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 10014-10037. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 5
                        },
                        {
                            "bbox": [
                                69,
                                571,
                                290,
                                615
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        571,
                                        290,
                                        615
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                571,
                                                290,
                                                615
                                            ],
                                            "type": "text",
                                            "content": "Logesh Kumar Umapathi, Ankit Pal, and Malaikannan Sankarasubbu. 2023. Med-halt: Medical domain hallucination test for large language models. CoRR, abs/2307.15343."
                                        }
                                    ]
                                }
                            ],
                            "index": 6
                        },
                        {
                            "bbox": [
                                69,
                                624,
                                291,
                                678
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        624,
                                        291,
                                        678
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                624,
                                                291,
                                                678
                                            ],
                                            "type": "text",
                                            "content": "Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. 2023. A stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation. CoRR, abs/2307.03987."
                                        }
                                    ]
                                }
                            ],
                            "index": 7
                        },
                        {
                            "bbox": [
                                69,
                                686,
                                291,
                                730
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        686,
                                        291,
                                        730
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                686,
                                                291,
                                                730
                                            ],
                                            "type": "text",
                                            "content": "David Wan, Shiyue Zhang, and Mohit Bansal. 2023. Histalign: Improving context dependency in language generation by aligning with history. CoRR, abs/2305.04782."
                                        }
                                    ]
                                }
                            ],
                            "index": 8
                        },
                        {
                            "bbox": [
                                69,
                                738,
                                291,
                                772
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        738,
                                        291,
                                        772
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                738,
                                                291,
                                                772
                                            ],
                                            "type": "text",
                                            "content": "Chaojun Wang and Rico Sennrich. 2020. On exposure bias, hallucination and domain shift in neural machine translation. In Proceedings of the 58th Annual"
                                        }
                                    ]
                                }
                            ],
                            "index": 9
                        }
                    ],
                    "sub_type": "ref_text"
                },
                {
                    "bbox": [
                        304,
                        72,
                        525,
                        772
                    ],
                    "type": "list",
                    "angle": 0,
                    "index": 21,
                    "blocks": [
                        {
                            "bbox": [
                                314,
                                72,
                                525,
                                116
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        314,
                                        72,
                                        525,
                                        116
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                314,
                                                72,
                                                525,
                                                116
                                            ],
                                            "type": "text",
                                            "content": "Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 3544-3552. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 11
                        },
                        {
                            "bbox": [
                                304,
                                126,
                                525,
                                191
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        126,
                                        525,
                                        191
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                126,
                                                525,
                                                191
                                            ],
                                            "type": "text",
                                            "content": "Junyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi, Chenlin Zhao, Haiyang Xu, Qinghao Ye, Ming Yan, Ji Zhang, Jihua Zhu, Jitao Sang, and Haoyu Tang. 2023a. Evaluation and analysis of hallucination in large vision-language models. CoRR, abs/2308.15126."
                                        }
                                    ]
                                }
                            ],
                            "index": 12
                        },
                        {
                            "bbox": [
                                304,
                                200,
                                525,
                                244
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        200,
                                        525,
                                        244
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                200,
                                                525,
                                                244
                                            ],
                                            "type": "text",
                                            "content": "Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023b. Large language models are not fair evaluators."
                                        }
                                    ]
                                }
                            ],
                            "index": 13
                        },
                        {
                            "bbox": [
                                304,
                                254,
                                525,
                                353
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        254,
                                        525,
                                        353
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                254,
                                                525,
                                                353
                                            ],
                                            "type": "text",
                                            "content": "Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022. OFA: unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 23318-23340. PMLR."
                                        }
                                    ]
                                }
                            ],
                            "index": 14
                        },
                        {
                            "bbox": [
                                304,
                                362,
                                525,
                                428
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        362,
                                        525,
                                        428
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                362,
                                                525,
                                                428
                                            ],
                                            "type": "text",
                                            "content": "Peng Wang, Ningyu Zhang, Xin Xie, Yunzhi Yao, Bozhong Tian, Mengru Wang, Zekun Xi, Siyuan Cheng, Kangwei Liu, Guozhou Zheng, and Huajun Chen. 2023c. EASYedit: An easy-to-use knowledge editing framework for large language models. CoRR, abs/2308.07269."
                                        }
                                    ]
                                }
                            ],
                            "index": 15
                        },
                        {
                            "bbox": [
                                304,
                                438,
                                525,
                                526
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        438,
                                        525,
                                        526
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                438,
                                                525,
                                                526
                                            ],
                                            "type": "text",
                                            "content": "Sirui Wang, Kaiwen Wei, Hongzhi Zhang, Yuntao Li, and Wei Wu. 2023d. Let me check the examples: Enhancing demonstration learning via explicit imitation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 1080-1088. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 16
                        },
                        {
                            "bbox": [
                                304,
                                534,
                                525,
                                590
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        534,
                                        525,
                                        590
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                534,
                                                525,
                                                590
                                            ],
                                            "type": "text",
                                            "content": "Xintao Wang, Qianwen Yang, Yongting Qiu, Jiaqing Liang, Qianyu He, Zhouhong Gu, Yanghua Xiao, and Wei Wang. 2023e. Knowledgegpt: Enhancing large language models with retrieval and storage access on knowledge bases. CoRR, abs/2308.11761."
                                        }
                                    ]
                                }
                            ],
                            "index": 17
                        },
                        {
                            "bbox": [
                                304,
                                599,
                                525,
                                654
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        599,
                                        525,
                                        654
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                599,
                                                525,
                                                654
                                            ],
                                            "type": "text",
                                            "content": "Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and Qun Liu. 2023f. Aligning large language models with human: A survey. CoRR, abs/2307.12966."
                                        }
                                    ]
                                }
                            ],
                            "index": 18
                        },
                        {
                            "bbox": [
                                304,
                                664,
                                525,
                                718
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        664,
                                        525,
                                        718
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                664,
                                                525,
                                                718
                                            ],
                                            "type": "text",
                                            "content": "Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. 2023g. Unleashing cognitive synergy in large language models: A task-solving agent through multi-persona self-collaboration. CoRR, abs/2307.05300."
                                        }
                                    ]
                                }
                            ],
                            "index": 19
                        },
                        {
                            "bbox": [
                                304,
                                728,
                                525,
                                772
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        728,
                                        525,
                                        772
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                728,
                                                525,
                                                772
                                            ],
                                            "type": "text",
                                            "content": "Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022a. Finetuned language models are zero-shot learners. In The Tenth"
                                        }
                                    ]
                                }
                            ],
                            "index": 20
                        }
                    ],
                    "sub_type": "ref_text"
                }
            ],
            "discarded_blocks": [],
            "page_size": [
                595,
                841
            ],
            "page_idx": 18
        },
        {
            "para_blocks": [
                {
                    "bbox": [
                        69,
                        72,
                        291,
                        772
                    ],
                    "type": "list",
                    "angle": 0,
                    "index": 11,
                    "blocks": [
                        {
                            "bbox": [
                                80,
                                72,
                                291,
                                105
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        80,
                                        72,
                                        291,
                                        105
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                80,
                                                72,
                                                291,
                                                105
                                            ],
                                            "type": "text",
                                            "content": "International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net."
                                        }
                                    ]
                                }
                            ],
                            "index": 0
                        },
                        {
                            "bbox": [
                                69,
                                112,
                                290,
                                168
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        112,
                                        290,
                                        168
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                112,
                                                290,
                                                168
                                            ],
                                            "type": "text",
                                            "content": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022b. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS."
                                        }
                                    ]
                                }
                            ],
                            "index": 1
                        },
                        {
                            "bbox": [
                                69,
                                174,
                                290,
                                219
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        174,
                                        290,
                                        219
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                174,
                                                290,
                                                219
                                            ],
                                            "type": "text",
                                            "content": "Wenjing Yue Wei Zhu and Xiaoling Wang. 2023. Shennong-tcm: A traditional chinese medicine large language model. https://github.com/michael-wzhu/ShenNong-TCM-LLM."
                                        }
                                    ]
                                }
                            ],
                            "index": 2
                        },
                        {
                            "bbox": [
                                69,
                                226,
                                290,
                                280
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        226,
                                        290,
                                        280
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                226,
                                                290,
                                                280
                                            ],
                                            "type": "text",
                                            "content": "Orion Weller, Marc Marone, Nathaniel Weir, Dawn J. Lawrie, Daniel Khashabi, and Benjamin Van Durme. 2023. \"according to ...\" prompting language models improves quoting from pre-training data. CoRR, abs/2305.13252."
                                        }
                                    ]
                                }
                            ],
                            "index": 3
                        },
                        {
                            "bbox": [
                                69,
                                288,
                                290,
                                332
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        288,
                                        290,
                                        332
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                288,
                                                290,
                                                332
                                            ],
                                            "type": "text",
                                            "content": "Yilin Wen, Zifeng Wang, and Jimeng Sun. 2023. Mindmap: Knowledge graph prompting sparks graph of thoughts in large language models. CoRR, abs/2308.09729."
                                        }
                                    ]
                                }
                            ],
                            "index": 4
                        },
                        {
                            "bbox": [
                                69,
                                339,
                                290,
                                383
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        339,
                                        290,
                                        383
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                339,
                                                290,
                                                383
                                            ],
                                            "type": "text",
                                            "content": "Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, and Bing Qin. 2023. Examining the inter-consistency of large language models: An in-depth analysis via debate. CoRR, abs/2305.11595."
                                        }
                                    ]
                                }
                            ],
                            "index": 5
                        },
                        {
                            "bbox": [
                                69,
                                391,
                                290,
                                445
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        391,
                                        290,
                                        445
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                391,
                                                290,
                                                445
                                            ],
                                            "type": "text",
                                            "content": "Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. CoRR, abs/2304.12244."
                                        }
                                    ]
                                }
                            ],
                            "index": 6
                        },
                        {
                            "bbox": [
                                69,
                                453,
                                290,
                                530
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        453,
                                        290,
                                        530
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                453,
                                                290,
                                                530
                                            ],
                                            "type": "text",
                                            "content": "Kevin Yang, Dan Klein, Nanyun Peng, and Yuandong Tian. 2023. DOC: improving long story coherence with detailed outline control. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 3378-3465. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 7
                        },
                        {
                            "bbox": [
                                69,
                                538,
                                290,
                                625
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        538,
                                        290,
                                        625
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                538,
                                                290,
                                                625
                                            ],
                                            "type": "text",
                                            "content": "Kevin Yang, Yuandong Tian, Nanyun Peng, and Dan Klein. 2022. Re3: Generating longer stories with recursive reprompting and revision. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 4393-4479. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 8
                        },
                        {
                            "bbox": [
                                69,
                                632,
                                290,
                                687
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        632,
                                        290,
                                        687
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                632,
                                                290,
                                                687
                                            ],
                                            "type": "text",
                                            "content": "Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. CoRR, abs/2305.10601."
                                        }
                                    ]
                                }
                            ],
                            "index": 9
                        },
                        {
                            "bbox": [
                                69,
                                694,
                                290,
                                772
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        694,
                                        290,
                                        772
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                694,
                                                290,
                                                772
                                            ],
                                            "type": "text",
                                            "content": "Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang. 2023. mplug-owl: Modularization empowers large language models with multimodality. CoRR, abs/2304.14178."
                                        }
                                    ]
                                }
                            ],
                            "index": 10
                        }
                    ],
                    "sub_type": "ref_text"
                },
                {
                    "bbox": [
                        304,
                        72,
                        525,
                        772
                    ],
                    "type": "list",
                    "angle": 0,
                    "index": 22,
                    "blocks": [
                        {
                            "bbox": [
                                304,
                                72,
                                525,
                                170
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        72,
                                        525,
                                        170
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                72,
                                                525,
                                                170
                                            ],
                                            "type": "text",
                                            "content": "Fan Yin, Jesse Vig, Philippe Laban, Shafiq Joty, Caiming Xiong, and Chien-Sheng Wu. 2023a. Did you read the instructions? rethinking the effectiveness of task definitions in instruction learning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 3063-3079. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 12
                        },
                        {
                            "bbox": [
                                304,
                                179,
                                525,
                                222
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        179,
                                        525,
                                        222
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                179,
                                                525,
                                                222
                                            ],
                                            "type": "text",
                                            "content": "Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. 2023b. A survey on multimodal large language models. CoRR, abs/2306.13549."
                                        }
                                    ]
                                }
                            ],
                            "index": 13
                        },
                        {
                            "bbox": [
                                304,
                                231,
                                525,
                                351
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        231,
                                        525,
                                        351
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                231,
                                                525,
                                                351
                                            ],
                                            "type": "text",
                                            "content": "Jifan Yu, Xiaozhi Wang, Shangqing Tu, Shulin Cao, Daniel Zhang-li, Xin Lv, Hao Peng, Zijun Yao, Xiaohan Zhang, Hanming Li, Chunyang Li, Zheyuan Zhang, Yushi Bai, Yantao Liu, Amy Xin, Nianyi Lin, Kaifeng Yun, Linlu Gong, Jianhui Chen, Zhili Wu, Yunjia Qi, Weikai Li, Yong Guan, Kaisheng Zeng, Ji Qi, Hailong Jin, Jinxin Liu, Yu Gu, Yuan Yao, Ning Ding, Lei Hou, Zhiyuan Liu, Bin Xu, Jie Tang, and Juanzi Li. 2023a. Kola: Carefully benchmarking world knowledge of large language models. CoRR, abs/2306.09296."
                                        }
                                    ]
                                }
                            ],
                            "index": 14
                        },
                        {
                            "bbox": [
                                304,
                                359,
                                525,
                                404
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        359,
                                        525,
                                        404
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                359,
                                                525,
                                                404
                                            ],
                                            "type": "text",
                                            "content": "Wenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng Jiang, and Ashish Sabharwal. 2023b. Improving language models via plug-and-play retrieval feedback. CoRR, abs/2305.14002."
                                        }
                                    ]
                                }
                            ],
                            "index": 15
                        },
                        {
                            "bbox": [
                                304,
                                412,
                                525,
                                456
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        412,
                                        525,
                                        456
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                412,
                                                525,
                                                456
                                            ],
                                            "type": "text",
                                            "content": "Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, and Meng Jiang. 2022. A survey of knowledge-enhanced text generation. ACM Comput. Surv., 54(11s):227:1-227:38."
                                        }
                                    ]
                                }
                            ],
                            "index": 16
                        },
                        {
                            "bbox": [
                                304,
                                464,
                                525,
                                497
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        464,
                                        525,
                                        497
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                464,
                                                525,
                                                497
                                            ],
                                            "type": "text",
                                            "content": "Shuzhou Yuan and Michael Farber. 2023. Evaluating generative models for graph-to-text generation. CoRR, abs/2307.14712."
                                        }
                                    ]
                                }
                            ],
                            "index": 17
                        },
                        {
                            "bbox": [
                                304,
                                505,
                                525,
                                572
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        505,
                                        525,
                                        572
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                505,
                                                525,
                                                572
                                            ],
                                            "type": "text",
                                            "content": "Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text generation. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 27263-27277."
                                        }
                                    ]
                                }
                            ],
                            "index": 18
                        },
                        {
                            "bbox": [
                                304,
                                579,
                                525,
                                678
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        579,
                                        525,
                                        678
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                579,
                                                525,
                                                678
                                            ],
                                            "type": "text",
                                            "content": "Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie Tang. 2023. GLM-130B: an open bilingual pre-trained model. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net."
                                        }
                                    ]
                                }
                            ],
                            "index": 19
                        },
                        {
                            "bbox": [
                                304,
                                686,
                                525,
                                719
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        686,
                                        525,
                                        719
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                686,
                                                525,
                                                719
                                            ],
                                            "type": "text",
                                            "content": "Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A. Smith. 2023a. How language model hallucinations can snowball. CoRR, abs/2305.13534."
                                        }
                                    ]
                                }
                            ],
                            "index": 20
                        },
                        {
                            "bbox": [
                                304,
                                727,
                                525,
                                772
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        304,
                                        727,
                                        525,
                                        772
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                304,
                                                727,
                                                525,
                                                772
                                            ],
                                            "type": "text",
                                            "content": "Shuo Zhang, Liangming Pan, Junzhou Zhao, and William Yang Wang. 2023b. Mitigating language model hallucination with interactive question-knowledge alignment. CoRR, abs/2305.13669."
                                        }
                                    ]
                                }
                            ],
                            "index": 21
                        }
                    ],
                    "sub_type": "ref_text"
                }
            ],
            "discarded_blocks": [],
            "page_size": [
                595,
                841
            ],
            "page_idx": 19
        },
        {
            "para_blocks": [
                {
                    "bbox": [
                        69,
                        72,
                        291,
                        772
                    ],
                    "type": "list",
                    "angle": 0,
                    "index": 10,
                    "blocks": [
                        {
                            "bbox": [
                                69,
                                72,
                                291,
                                160
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        72,
                                        291,
                                        160
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                72,
                                                291,
                                                160
                                            ],
                                            "type": "text",
                                            "content": "Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT: open pre-trained transformer language models. CoRR, abs/2205.01068."
                                        }
                                    ]
                                }
                            ],
                            "index": 0
                        },
                        {
                            "bbox": [
                                69,
                                171,
                                291,
                                206
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        171,
                                        291,
                                        206
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                171,
                                                291,
                                                206
                                            ],
                                            "type": "text",
                                            "content": "Yifan Zhang, Jingqin Yang, Yang Yuan, and Andrew Chi-Chih Yao. 2023c. Cumulative reasoning with large language models. CoRR, abs/2308.04371."
                                        }
                                    ]
                                }
                            ],
                            "index": 1
                        },
                        {
                            "bbox": [
                                69,
                                216,
                                291,
                                305
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        216,
                                        291,
                                        305
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                216,
                                                291,
                                                305
                                            ],
                                            "type": "text",
                                            "content": "Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, and Lidong Bing. 2023a. Verify-and-edit: A knowledge-enhanced chain-of-thought framework. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 5823-5840. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 2
                        },
                        {
                            "bbox": [
                                69,
                                316,
                                291,
                                361
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        316,
                                        291,
                                        361
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                316,
                                                291,
                                                361
                                            ],
                                            "type": "text",
                                            "content": "Theodore Zhao, Mu Wei, J. Samuel Preston, and Hoifung Poon. 2023b. Llm calibration and automatic hallucination detection via pareto optimal self-supervision."
                                        }
                                    ]
                                }
                            ],
                            "index": 3
                        },
                        {
                            "bbox": [
                                69,
                                371,
                                291,
                                438
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        371,
                                        291,
                                        438
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                371,
                                                291,
                                                438
                                            ],
                                            "type": "text",
                                            "content": "Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023a. Judging llm-as-a-judge with mt-bench and chatbot arena. CoRR, abs/2306.05685."
                                        }
                                    ]
                                }
                            ],
                            "index": 4
                        },
                        {
                            "bbox": [
                                69,
                                449,
                                291,
                                483
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        449,
                                        291,
                                        483
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                449,
                                                291,
                                                483
                                            ],
                                            "type": "text",
                                            "content": "Shen Zheng, Jie Huang, and Kevin Chen-Chuan Chang. 2023b. Why does chatgpt fall short in answering questions faithfully? CoRR, abs/2304.10513."
                                        }
                                    ]
                                }
                            ],
                            "index": 5
                        },
                        {
                            "bbox": [
                                69,
                                494,
                                291,
                                550
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        494,
                                        291,
                                        550
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                494,
                                                291,
                                                550
                                            ],
                                            "type": "text",
                                            "content": "Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. LIMA: less is more for alignment. CoRR, abs/2305.11206."
                                        }
                                    ]
                                }
                            ],
                            "index": 6
                        },
                        {
                            "bbox": [
                                69,
                                560,
                                290,
                                606
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        560,
                                        290,
                                        606
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                560,
                                                290,
                                                606
                                            ],
                                            "type": "text",
                                            "content": "Deyao Zhu, Jun Chen, Xiaogian Shen, Xiang Li, and Mohamed Elhoseiny. 2023a. Minigpt-4: Enhancing vision-language understanding with advanced large language models. CoRR, abs/2304.10592."
                                        }
                                    ]
                                }
                            ],
                            "index": 7
                        },
                        {
                            "bbox": [
                                69,
                                617,
                                291,
                                717
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        617,
                                        291,
                                        717
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                617,
                                                291,
                                                717
                                            ],
                                            "type": "text",
                                            "content": "Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Yongfeng Huang, Ruyi Gan, Jiaxing Zhang, and Yu-jiu Yang. 2023b. Solving math word problems via cooperative reasoning induced language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 4471-4485. Association for Computational Linguistics."
                                        }
                                    ]
                                }
                            ],
                            "index": 8
                        },
                        {
                            "bbox": [
                                69,
                                727,
                                291,
                                772
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        69,
                                        727,
                                        291,
                                        772
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                69,
                                                727,
                                                291,
                                                772
                                            ],
                                            "type": "text",
                                            "content": "Xuekai Zhu, Biqing Qi, Kaiyan Zhang, Xingwei Long, and Bowen Zhou. 2023c. Pad: Program-aided distillation specializes large models in reasoning. CoRR, abs/2305.13888."
                                        }
                                    ]
                                }
                            ],
                            "index": 9
                        }
                    ],
                    "sub_type": "ref_text"
                },
                {
                    "bbox": [
                        304,
                        72,
                        526,
                        128
                    ],
                    "type": "ref_text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                304,
                                72,
                                526,
                                128
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        304,
                                        72,
                                        526,
                                        128
                                    ],
                                    "type": "text",
                                    "content": "Yuqi Zhu, Xiaohan Wang, Jing Chen, Shuofei Qiao, Yixin Ou, Yunzhi Yao, Shumin Deng, Huajun Chen, and Ningyu Zhang. 2023d. Llms for knowledge graph construction and reasoning: Recent capabilities and future opportunities. CoRR, abs/2305.13168."
                                }
                            ]
                        }
                    ],
                    "index": 11
                }
            ],
            "discarded_blocks": [],
            "page_size": [
                595,
                841
            ],
            "page_idx": 20
        }
    ],
    "_backend": "vlm",
    "_version_name": "2.5.3"
}